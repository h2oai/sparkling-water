apply plugin: 'base'
apply plugin: 'com.bmuschko.docker-remote-api'
apply from: "$rootDir/gradle/utils.gradle"

import com.bmuschko.gradle.docker.tasks.image.Dockerfile

defaultTasks 'createDockerFile'
description = "Create a Docker file for jenkins tests"

ext {
  outputFile = file("$buildDir/docker/Dockerfile")
  terraformDownloadUrl = "https://releases.hashicorp.com/terraform/${terraformVersion}/terraform_${terraformVersion}_linux_amd64.zip"
}

task copyFiles(type: Copy) {
  from "$projectDir/docker/conf"
  into "$buildDir/docker/conf"
}

task createDockerfile(type: Dockerfile, dependsOn: copyFiles) {
  destFile = outputFile
  from testingBaseImage

  runCommand "sudo yum update -y && sudo yum install -y yum-utils java-1.8.0-openjdk-devel unzip ca-certificates git docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin"

  // Install Terraform
  runCommand "sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo && sudo yum -y install terraform"

//  runCommand """\\
//                |   curl -s ${terraformDownloadUrl} --output terraform.zip && \\
//                |   unzip terraform.zip -d /usr/local/bin/ && \\
//                |   rm -f terraform.zip
//               """.stripMargin()

//  environmentVariable("PATH","/usr/lib/jvm/java-8-oracle/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin")

//  // Update to R 4.2
//  runCommand """\\
//                apt install software-properties-common -y && \\
//                apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 && \\
//                add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu xenial-cran40/' && \\
//                apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/3bf863cc.pub && \\
//                apt update && \\
//                apt install r-base -y && \\
//                R -e 'update.packages(ask = FALSE, checkBuilt = TRUE)' && \\
//                R -e 'install.packages("RCurl", repos = "http://cran.us.r-project.org")'
//                """
//
//  runCommand """\\
//                apt-get install libharfbuzz-dev libfribidi-dev && \\
//                R -e 'install.packages("testthat", repos = "http://cran.us.r-project.org")' && \\
//                R -e 'install.packages("dbplyr", repos = "http://cran.us.r-project.org")' && \\
//                R -e 'install.packages("sparklyr", repos = "http://cran.us.r-project.org")' && \\
//                R -e 'install.packages("devtools", repos = "http://cran.us.r-project.org")'
//                """

  runCommand("sudo useradd jenkins")
  user("jenkins")
  getAllFullSparkVersions().each { version ->
    String tgzFileName
    String jenkinsHome = "/home/jenkins"
    String sparkFolder = "spark-${version}-bin"
    String sparkPath = "${jenkinsHome}/${sparkFolder}"
    if (version > '3.4') {
      tgzFileName = "spark-${version}-bin-hadoop3.tgz"
    } else if (version > '3.3') {
      tgzFileName = "spark-${version}-bin-hadoop2.tgz"
    } else {
      tgzFileName = "spark-${version}-bin-hadoop2.7.tgz"
    }
    runCommand """\\
                    cd ${jenkinsHome} && \\
                    wget http://archive.apache.org/dist/spark/spark-${version}/${tgzFileName} && \\
                    mkdir -p ${sparkFolder} &&  \\
                    tar zxvf ${tgzFileName} -C ${sparkFolder} --strip-components 1 && \\
                    rm -rf ${tgzFileName}
                    """

    if (version.startsWith("2.4")) {
      runCommand "sed -i 's/openjdk:8-jdk-slim/openjdk:8-jdk-slim-buster/g' ${sparkPath}/kubernetes/dockerfiles/spark/Dockerfile"
    }
    if (version.startsWith("3.0") || version.startsWith("3.1")) {
      runCommand "sed -i 's/\\(apt-key[^)]*\\)/apt-key adv --keyserver keyserver.ubuntu.com --recv-key '95C0FAF38DB3CCAD0C080A7BDC78B2DDEABC47B7'/g' ${sparkPath}/kubernetes/dockerfiles/spark/bindings/R/Dockerfile"
    }
    if (version > "3.4") { //downgrade back to openjdk as we do not support JDK 17 (yet)
      runCommand "sed -i 's/eclipse-temurin/openjdk/g' ${sparkPath}/kubernetes/dockerfiles/spark/Dockerfile"
    }
    def first = version.split("\\.")[0]
    def second = version.split("\\.")[1]
    environmentVariable("SPARK_HOME_${first}_${second}", sparkPath)
  }

  def branchName = getGitBranch()

  environmentVariable("USER", "jenkins")
  runCommand """\\
               cd /home/jenkins && \\
               wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /home/jenkins/miniconda.sh && \\
               bash /home/jenkins/miniconda.sh -b -p /home/jenkins/miniconda && \\
               rm /home/jenkins/miniconda.sh
                """

//  environmentVariable("PATH", "/home/jenkins/miniconda/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin")
//  environmentVariable("JAVA_HOME", "/usr/lib/jvm/java-8-oracle")

  runCommand """\\
                . ~/miniconda/etc/profile.d/conda.sh && \\
                conda install anaconda-client conda-build conda-verify -y && \\
                conda update conda -y && \\
                conda update anaconda-client conda-build conda-verify -y && \\
                conda install -n base conda-libmamba-solver && \\
                conda config --set solver libmamba && \\
                conda config --add channels conda-forge && \\
                conda init
                """

  runCommand """\\
                . ~/miniconda/etc/profile.d/conda.sh && \\
                for pythonVersion in ${pythonEnvironments}; \\
                do \\
                  conda create -n sw_env_python\$pythonVersion python=\$pythonVersion -y && \\
                  conda activate sw_env_python\$pythonVersion && \\
                  conda install setuptools -y && \\
                  conda install twine -y && \\
                  conda install virtualenv -y && \\
                  conda deactivate; \\
                done
                """

  runCommand """\\
                cd /home/jenkins && \\
                git clone https://github.com/h2oai/sparkling-water.git && \\
                cd sparkling-water && \\
                git checkout ${branchName} && \\
                for sparkMajor in ${supportedSparkVersions}; \\
                do \\
                  ./gradlew -Pspark=\$sparkMajor --refresh-dependencies resolveDependencies && \\
                  for pythonVersion in ${supportedPythonVersions}; \\
                  do \\
                    ./gradlew -Pspark=\$sparkMajor :sparkling-water-py:pipInstall -PpythonPath=/home/jenkins/miniconda/envs/sw_env_python\$pythonVersion/bin -PpythonEnvBasePath=/home/jenkins/.gradle/python; \\
                  done; \\ 
                done && cd .. && rm -rf sparkling-water
                """

  user("root")

  runCommand """\\
                sudo sh -c "echo \\"jenkins ALL=(ALL) NOPASSWD:ALL\\" >> /etc/sudoers"
               """

  user("jenkins")

  // install kubectl
  runCommand """\\
                cd /home/jenkins && \\
                curl -LOvk "https://dl.k8s.io/release/`curl -Lsk https://dl.k8s.io/release/stable.txt`/bin/linux/amd64/kubectl" && \\
                sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl && \\
                rm kubectl
             """

  // install minikube
  runCommand """\\
                cd /home/jenkins && \\
                curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && \\
                sudo install minikube-linux-amd64 /usr/local/bin/minikube && \\
                rm minikube-linux-amd64
             """

  // install aws cli
  runCommand """\\
                cd /home/jenkins && \\
                curl https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip && \\
                unzip awscliv2.zip -d awscliv2 && \\
                sudo ./awscliv2/aws/install -i /usr/local/aws-cli -b /usr/local/bin && \\
                rm -rf awscliv2
             """
}

def getAllFullSparkVersions() {
  return supportedSparkVersions.split(" ").collect { majorVersion ->
    def props = new Properties()
    file("$rootDir/gradle-spark${majorVersion}.properties").withInputStream { props.load(it) }
    props.get("sparkVersion").toString()
  }
}

task cleanDockerfile(type: Delete) {
  delete outputFile
}

clean.dependsOn cleanDockerfile

task dockerRepoLoginCommand {
  doLast {
    def registryId = file("$rootDir/ci/aws/terraform/infra.properties").readLines().find {it.startsWith("docker_registry_id")}.split("=")[1]
    println("aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin ${registryId}.dkr.ecr.us-west-2.amazonaws.com")
  }
}
