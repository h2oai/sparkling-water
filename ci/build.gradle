apply plugin: 'base'
apply plugin: 'com.bmuschko.docker-remote-api'

import com.bmuschko.gradle.docker.tasks.image.Dockerfile

defaultTasks 'createDockerFile'
description = "Create a Docker file for jenkins tests"

ext {
  outputFile = file("$buildDir/docker/Dockerfile")
  terraformDownloadUrl = "https://releases.hashicorp.com/terraform/${terraformVersion}/terraform_${terraformVersion}_linux_amd64.zip"
}

task copyFiles(type: Copy) {
  from "$projectDir/docker/conf"
  into "$buildDir/docker/conf"
}

task createDockerfile(type: Dockerfile, dependsOn: copyFiles) {
  destFile = outputFile
  from 'harbor.h2o.ai/opsh2oai/h2o-3-hadoop-hdp-2.2:76'

  environmentVariable("LANG", "'C.UTF-8'")
  runCommand "locale"
  runCommand "rm -rf /etc/hadoop/conf/yarn-site.xml"
  copyFile("conf/yarn-site.xml", "/etc/hadoop/conf/yarn-site.xml")
  runCommand "rm /etc/startup/70_start_slapd"

  // Install Terraform
  runCommand """\\
                |   curl -s ${terraformDownloadUrl} --output terraform.zip && \\
                |   unzip terraform.zip -d /usr/local/bin/ && \\
                |   rm -f terraform.zip
               """.stripMargin()

  runCommand """\\
                R -e 'install.packages("testthat", repos = "http://cran.us.r-project.org")' && \\
                R -e 'install.packages("sparklyr", repos = "http://cran.us.r-project.org")'
                """
  user("jenkins")
  getAllFullSparkVersions().each { version ->
    runCommand """\\
                    cd /home/jenkins && \\
                    wget http://archive.apache.org/dist/spark/spark-${version}/spark-${version}-bin-hadoop2.7.tgz  && \\
                    mkdir -p spark-${version}-bin-hadoop2.7 &&  \\
                    tar zxvf spark-${version}-bin-hadoop2.7.tgz -C spark-${version}-bin-hadoop2.7 --strip-components 1 && \\
                    rm -rf spark-${version}-bin-hadoop2.7.tgz
                    """

    def first = version.split("\\.")[0]
    def second = version.split("\\.")[1]
    environmentVariable("SPARK_HOME_${first}_${second}", "/home/jenkins/spark-${version}-bin-hadoop2.7")
  }
  runCommand """\\
                cd /home/jenkins && \\
                git clone https://github.com/h2oai/sparkling-water.git && \\
                cd sparkling-water && \\
                for sparkMajor in ${supportedSparkVersions}; \\
                do \\
                  ./gradlew -Pspark=\$sparkMajor --refresh-dependencies resolveDependencies && \\
                  ./gradlew -Pspark=\$sparkMajor :sparkling-water-py:pipInstall -PpythonPath=/envs/h2o_env_python2.7/bin -PpythonEnvBasePath=/home/jenkins/.gradle/python && \\
                  ./gradlew -Pspark=\$sparkMajor :sparkling-water-py:pipInstall -PpythonPath=/envs/h2o_env_python3.6/bin -PpythonEnvBasePath=/home/jenkins/.gradle/python; \\
                done && cd .. && rm -rf sparkling-water
                """

  user("root")

  runCommand """\\
                sudo sh -c "echo \\"jenkins ALL=(ALL) NOPASSWD:ALL\\" >> /etc/sudoers"
               """

  user("jenkins")
  environmentVariable("USER", "jenkins")
  runCommand """\\
               cd /home/jenkins && \\
               wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /home/jenkins/miniconda.sh && \\
               bash /home/jenkins/miniconda.sh -b -p /home/jenkins/miniconda && \\
               rm /home/jenkins/miniconda.sh
                """
  environmentVariable("PATH", "\"/home/jenkins/miniconda/bin:\${PATH}\"")
  runCommand """\\
                conda install anaconda anaconda-client conda-build -y && \\
                conda update conda -y && \\
                conda update anaconda anaconda-client conda-build -y && \\
                conda config --add channels conda-forge
                """

  environmentVariable("HIVE_HOME", "/usr/hdp/2.2.9.0-3393/hive")
  runCommand "pip install awscli --upgrade --user"
}

def getAllFullSparkVersions() {
  return supportedSparkVersions.split(" ").collect { majorVersion ->
    def props = new Properties()
    file("$rootDir/gradle-spark${majorVersion}.properties").withInputStream { props.load(it) }
    props.get("sparkVersion").toString()
  }
}

task cleanDockerfile(type: Delete) {
  delete outputFile
}

clean.dependsOn cleanDockerfile

task dockerRepoLoginCommand {
  doLast {
    def registryId = file("$rootDir/ci/aws/terraform/infra.properties").readLines().find {it.startsWith("docker_registry_id")}.split("=")[1]
    println("aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin ${registryId}.dkr.ecr.us-west-2.amazonaws.com")
  }
}
