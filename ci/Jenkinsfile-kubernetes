#!/usr/bin/groovy
@Library('test-shared-library') _

properties(
        [
                pipelineTriggers([cron('H 16 * * *')]),
                buildDiscarder(logRotator(numToKeepStr: '10'))
        ]
)

def readPropertiesFile(file) {
    def properties = [:]
    readFile(file).split("\n").each { line ->
        if (!line.startsWith("#")) {
            def splits = line.split("=")
            properties[splits[0]] = splits[1]
        }
    }
    return properties
}

def getKubernetesSparkVersions(props) {
    def sparkVersions = props["supportedSparkVersions"].split(" ").toList()
    def boundaryVersion = props["kubernetesSupportSinceSpark"]
    def list = new ArrayList<String>()
    list.addAll(sparkVersions.subList(sparkVersions.indexOf(boundaryVersion), sparkVersions.size()))
    return list
}

def getSparklingVersion(props, sparkMajorVersion) {
    return "${props['version'].replace("-SNAPSHOT", "")}-${sparkMajorVersion}"
}

String getSparkVersion(sparkMajorVersion) {
    def versionLine = readFile("gradle-spark${sparkMajorVersion}.properties").split("\n").find() { line -> line.startsWith('sparkVersion') }
    return versionLine.split("=")[1]
}

def getBuildAndPublishStage(props, sparkMajorVersion) {
    return {
        stage("Build & Publish Images For Spark $sparkMajorVersion") {
            node('docker') {
                ws("${env.WORKSPACE}-spark-${sparkMajorVersion}") {
                    cleanWs()
                    checkout scm
                    def commons = load 'ci/commons.groovy'
                    commons.withSparklingWaterDockerImage {
                        sh "sudo apt -y install docker.io"
                        sh "sudo service docker start"
                        sh "sudo chmod 666 /var/run/docker.sock"
                        def sparklingVersion = getSparklingVersion(props, sparkMajorVersion)
                        sh "./gradlew dist -Pspark=$sparkMajorVersion"
                        def customEnv = [
                                "SPARK_HOME=/home/jenkins/spark-${getSparkVersion(sparkMajorVersion)}-bin-hadoop2.7",
                        ]
                        withEnv(customEnv) {
                            dir("./dist/build/zip/sparkling-water-${sparklingVersion}") {
                                sh """
                                    ./bin/build-kubernetes-images.sh scala
                                    ./bin/build-kubernetes-images.sh python
                                    ./bin/build-kubernetes-images.sh r
                                   """
                            }
                        }

                        unstash "properties"
                        def repoId = readPropertiesFile("properties")["docker_registry_id"]
                        def repoUrl = "${repoId}.dkr.ecr.us-east-2.amazonaws.com"
                        docker.withRegistry("https://${repoUrl}", 'ecr:us-east-2:SW_FULL_AWS_CREDS') {
                            sh """
                            docker tag sparkling-water-r:${sparklingVersion} $repoUrl/sw_kubernetes_repo/sparkling-water:r-${sparklingVersion}
                            docker tag sparkling-water-python:${sparklingVersion} $repoUrl/sw_kubernetes_repo/sparkling-water:python-${sparklingVersion}
                            docker tag sparkling-water-scala:${sparklingVersion} $repoUrl/sw_kubernetes_repo/sparkling-water:scala-${sparklingVersion}
                            docker push $repoUrl/sw_kubernetes_repo/sparkling-water:r-${sparklingVersion}
                            docker push $repoUrl/sw_kubernetes_repo/sparkling-water:python-${sparklingVersion}
                            docker push $repoUrl/sw_kubernetes_repo/sparkling-water:scala-${sparklingVersion}
                            """
                        }
                    }
                    cleanWs()
                }
            }
        }
    }
}

def getBuildAndPublishStages(props) {
    def parallelStages = [:]
    getKubernetesSparkVersions(props).each { sparkMajorVersion ->
        parallelStages["Build & Publish Spark ${sparkMajorVersion}"] = getBuildAndPublishStage(props, sparkMajorVersion)
    }
    return parallelStages
}


def withKubectl(commons, sparkMajorVersion, code) {
    commons.withSparklingWaterDockerImage {
        commons.withAWSCredentials {
            unstash "properties"
            sh """
                sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2
                curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
                echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
                sudo apt-get update
                sudo apt-get install -y kubectl
                kubectl version --client
                sudo cp ~/.local/bin/aws /usr/local/bin/aws
            """
            sh """
                aws eks --region us-east-2 update-kubeconfig --name ${readPropertiesFile("properties")["cluster_name"]}
                kubectl get svc
                kubectl create clusterrolebinding default --clusterrole=edit --serviceaccount=default:default --namespace=default
                """
            def customEnv = [
                    "SPARK_HOME=/home/jenkins/spark-${getSparkVersion(sparkMajorVersion)}-bin-hadoop2.7",
            ]
            withEnv(customEnv) {
                code()
            }
        }
    }
}
node("docker") {
    cleanWs()
    checkout scm
    def commons = load 'ci/commons.groovy'
    def props = readPropertiesFile("gradle.properties")
    def sparklingVersion = props['version'].replace("-SNAPSHOT", "")

    stage("Start EKS & ECR") {
        commons.withAWSCredentials {
            dir("kubernetes/src/terraform/aws") {
                def code = {
                    commons.terraformApply()
                    def values = commons.extractTerraformOutputs(["docker_registry_id", "cluster_name"])
                    def valuesAsString = values.collect { "${it.key}=${it.value}" }.join("\n") + "\n" + "version=$sparklingVersion" + "\n"
                    writeFile file: "properties", text: valuesAsString
                    stash name: "properties", includes: "properties"
                    arch "terraform.tfstate"
                }
                commons.withDocker("hashicorp/terraform:0.12.25", code, "--entrypoint='' --network host")
            }
        }
    }

    parallel(getBuildAndPublishStages(props))

    getKubernetesSparkVersions(props).each { sparkMajorVersion ->
        stage("Test internal backend, Spark ${sparkMajorVersion}") {
            withKubectl(commons, sparkMajorVersion) {
                def registryId = readPropertiesFile("properties")["docker_registry_id"]
                def version = readPropertiesFile("properties")["version"] + "-${sparkMajorVersion}"
                def master =  "k8s://" + sh(
                        script: 'kubectl config view -o jsonpath="{.clusters[0].cluster.server}',
                        returnStdout: true
                ).trim()
                // Test Scala
                sh """
                    \$SPARK_HOME/bin/spark-submit \
                     --conf spark.kubernetes.container.image=${registryId}.dkr.ecr.us-east-2.amazonaws.com/sw_kubernetes_repo/sparkling-water:scala-${version} \
                     --conf spark.kubernetes.driver.pod.name=driver-scala
                     --conf spark.scheduler.minRegisteredResourcesRatio=1
                     --master $master \
                     --deploy-mode cluster \
                     --name test \
                     --class ai.h2o.sparkling.InitTest \
                     --conf spark.executor.instances=3 \
                     local:///opt/sparkling-water/tests/initTest.jar
                    """
                // Check exit code of the pod
                sh "kubectl get pod driver-scala | grep -q && echo \"OK\" || exit 1"
                // Test Python
                sh """
                    \$SPARK_HOME/bin/spark-submit \
                     --conf spark.kubernetes.container.image=${registryId}.dkr.ecr.us-east-2.amazonaws.com/sw_kubernetes_repo/sparkling-water:python-${version} \
                     --conf spark.kubernetes.driver.pod.name=driver-python
                     --conf spark.scheduler.minRegisteredResourcesRatio=1
                     --master $master \
                     --deploy-mode cluster \
                     --name test \
                     --conf spark.executor.instances=3 \
                     local:///opt/sparkling-water/tests/initTest.py
                    """
                sh "kubectl get pod driver-python | grep -q && echo \"OK\" || exit 1"

                // Test R
                sh """
                    R CMD BATCH submit.R $master $registryId $version
                    """
            }
        }
    }

    stage("Stop EKS & ECR") {
        commons.withAWSCredentials {
            dir("kubernetes/src/terraform/aws") {
                def code = {
                    commons.terraformDestroy()
                }
                //commons.withDocker("hashicorp/terraform:0.12.25", code, "--entrypoint='' --network host")
            }
        }
    }
}
