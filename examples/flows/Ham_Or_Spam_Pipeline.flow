{
  "version": "1.0.0",
  "cells": [
    {
      "type": "cs",
      "input": "importFiles"
    },
    {
      "type": "cs",
      "input": "importFiles [ \"https://raw.githubusercontent.com/h2oai/sparkling-water/master/examples/smalldata/smsData.txt\" ]"
    },
    {
      "type": "cs",
      "input": "setupParse source_frames: [ \"https://raw.githubusercontent.com/h2oai/sparkling-water/master/examples/smalldata/smsData.txt\" ]"
    },
    {
      "type": "cs",
      "input": "parseFiles\n  source_frames: [\"https://raw.githubusercontent.com/h2oai/sparkling-water/master/examples/smalldata/smsData.txt\"]\n  destination_frame: \"smsData.hex\"\n  parse_type: \"CSV\"\n  separator: 9\n  number_columns: 2\n  single_quotes: false\n  column_names: null\n  column_names: [\"label\",\"text\"]\n  column_types: [\"Enum\",\"String\"]\n  delete_on_done: true\n  check_header: -1\n  chunk_size: 4194304"
    },
    {
      "type": "cs",
      "input": "asDataFrame \"smsData.hex\", \"spark_frame_data\""
    },
    {
      "type": "sca",
      "input": "import org.apache.spark.SparkFiles\nimport org.apache.spark.ml._\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.h2o.features._\nimport org.apache.spark.ml.h2o.algos._\nimport org.apache.spark.sql.types.{StringType, StructField, StructType}\nimport org.apache.spark.sql.{DataFrame, Row, SQLContext}\nimport water.support.SparkContextSupport\nimport water.fvec.H2OFrame"
    },
    {
      "type": "sca",
      "input": "// Create SQL support\nimplicit val sqlContext = SQLContext.getOrCreate(sc)\n// Start H2O services\nimport org.apache.spark.h2o._\nimplicit val h2oContext = H2OContext.getOrCreate(sc)"
    },
    {
      "type": "sca",
      "input": "\n/**\n  * Define the pipeline stages\n  */\n// Tokenize the messages\nval tokenizer = new RegexTokenizer().\n  setInputCol(\"text\").\n  setOutputCol(\"words\").\n  setMinTokenLength(3).\n  setGaps(false).\n  setPattern(\"[a-zA-Z]+\")\n"
    },
    {
      "type": "sca",
      "input": "// Remove ignored words\nval stopWordsRemover = new StopWordsRemover().\n  setInputCol(tokenizer.getOutputCol).\n  setOutputCol(\"filtered\").\n  setStopWords(Array(\"the\", \"a\", \"\", \"in\", \"on\", \"at\", \"as\", \"not\", \"for\")).\n  setCaseSensitive(false)"
    },
    {
      "type": "sca",
      "input": "// Hash the words\nval hashingTF = new HashingTF().\n  setNumFeatures(1 << 10).\n  setInputCol(tokenizer.getOutputCol).\n  setOutputCol(\"wordToIndex\")"
    },
    {
      "type": "sca",
      "input": "// Create inverse document frequencies model\nval idf = new IDF().\n  setMinDocFreq(4).\n  setInputCol(hashingTF.getOutputCol).\n  setOutputCol(\"tf_idf\")"
    },
    {
      "type": "sca",
      "input": "// Create H2ODeepLearning model\nval dl = new H2ODeepLearning().\n  setEpochs(10).\n  setL1(0.001).\n  setL2(0.0).\n  setHidden(Array[Int](200, 200)).\n  setFeaturesCols(idf.getOutputCol).\n  setPredictionsCol(\"label\")"
    },
    {
      "type": "sca",
      "input": "// Remove all intermediate columns\nval colPruner = new ColumnPruner().\n  setColumns(Array[String](idf.getOutputCol, hashingTF.getOutputCol, stopWordsRemover.getOutputCol, tokenizer.getOutputCol))"
    },
    {
      "type": "sca",
      "input": "// Create the pipeline by defining all the stages\nval pipeline = new Pipeline().\n  setStages(Array(tokenizer, stopWordsRemover, hashingTF, idf, dl, colPruner))"
    },
    {
      "type": "sca",
      "input": "// Train the pipeline model\nval data = sqlContext.table(\"spark_frame_data\")\nval model = pipeline.fit(data)"
    },
    {
      "type": "sca",
      "input": "/*\n * Make predictions on unlabeled data\n * Spam detector\n */\ndef isSpam(smsText: String,\n           model: PipelineModel,\n           h2oContext: H2OContext,\n           hamThreshold: Double = 0.5) = {\n  val smsTextSchema = StructType(Array(StructField(\"text\", StringType, nullable = false)))\n  val smsTextRowRDD = sc.parallelize(Seq(smsText)).map(Row(_))\n  val smsTextDF = sqlContext.createDataFrame(smsTextRowRDD, smsTextSchema)\n  val prediction = model.transform(smsTextDF)\n  prediction.select(\"spam\").first.getDouble(0) > hamThreshold\n}"
    },
    {
      "type": "sca",
      "input": "println(isSpam(\"Peter, what about a nice glass of vine after the meetup?\", model, h2oContext))\nprintln(isSpam(\"We tried to contact you re your reply to our offer of a Video Handset? 750 anytime any networks mins? UNLIMITED TEXT?\", model, h2oContext))\n"
    }
  ]
}