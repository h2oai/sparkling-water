description = "Sparkling Water Examples"

apply from: "$rootDir/gradle/utils.gradle"
apply from: "$rootDir/gradle/scriptsTest.gradle"

project.ext.exampleResources = project.file("flows")

sourceSets {
    main {
        resources {
            srcDir("flows")
        }
    }
}

jar {
    eachFile { f ->
        if (new File(exampleResources, f.file.name).exists()) {
            f.path = "www/flow/packs/examples/$f.name"
        }
    }
}

configurations {
    sparklingWaterAssemblyJar
}

dependencies {

    sparklingWaterAssemblyJar project(path: ':sparkling-water-assembly', configuration: 'shadow')
    api project(path: ':sparkling-water-assembly', configuration: 'shadow')

    // H2O HDFS Persistent layer
    api("ai.h2o:h2o-persist-hdfs:${h2oVersion}") {
        // Cannot use here: transitive = false since Gradle is producing wrong POM file
        // Hence the exclusions are listed manually
        exclude(group: "org.apache.hadoop", module: "hadoop-client")
        exclude(group: "org.apache.hadoop", module: "hadoop-hdfs-client")
        exclude(group: "org.apache.hadoop", module: "hadoop-aws")
    }


    // And Scala library
    api "org.scala-lang:scala-library:${scalaVersion}"

    // Add streaming
    compileOnly "org.apache.spark:spark-streaming_${scalaBaseVersion}:${sparkVersion}"
    testImplementation "org.apache.spark:spark-streaming_${scalaBaseVersion}:${sparkVersion}"

    testImplementation "org.apache.spark:spark-repl_${scalaBaseVersion}:${sparkVersion}"

    // And use scalatest for Scala testing
    testImplementation "org.scalatest:scalatest_${scalaBaseVersion}:2.2.1"
    testImplementation "junit:junit:4.11"
    // to reuse some utils test classes defined in the core
    testImplementation project(path: ':sparkling-water-core', configuration: 'testArchives')

    // Integration tests requirements
    integTestImplementation "org.scalatest:scalatest_${scalaBaseVersion}:2.2.1"
    integTestImplementation "junit:junit:4.11"
    // to reuse some utils test classes defined in the core
    integTestImplementation project(path: ':sparkling-water-core', configuration: 'testArchives')

    // To enable Idea compiler even for integTestCode which use mllib
    api "org.apache.spark:spark-mllib_${scalaBaseVersion}:${sparkVersion}"

    // Put Spark Assembly on runtime path
    integTestRuntimeOnly fileTree(dir: new File((String) sparkHome, "jars/"), include: '*.jar')
    scriptsTestRuntimeOnly fileTree(dir: new File((String) sparkHome, "jars/"), include: '*.jar')

}

integTest {
    systemProperty "spark.ext.h2o.backend.cluster.mode", detectBackendClusterMode()
    systemProperty "spark.testing", "true"
    systemProperty "spark.test.home", "${sparkHome}"
    systemProperty "sparkling.test.hdp.version", "${hdpVersion}"

    // Pass list of jars required for testing
    systemProperty "sparkling.assembly.jar", "${configurations.sparklingWaterAssemblyJar.singleFile}"
    systemProperty "sparkling.itest.jar", "${integTestJar.archiveFile}"
    systemProperty "spark.driver.extraJavaOptions", "-Dhdp.version=${hdpVersion}"
    systemProperty "spark.yarn.am.extraJavaOptions", "-Dhdp.version=${hdpVersion}"
    systemProperty "spark.executor.extraJavaOptions", "-Dhdp.version=${hdpVersion}"

    // Show standard out and standard error of the test JVM(s) on the console
    testLogging.showStandardStreams = true
}

// Setup test environment for scripts test
scriptsTest {
    systemProperty "spark.ext.h2o.backend.cluster.mode", detectBackendClusterMode()
    systemProperty "spark.testing", "true"
    systemProperty "spark.test.home", "${sparkHome}"
    systemProperty "sparkling.test.log.dir", new File(project.getBuildDir(), "h2ologs-test")
    systemProperty "spark.ext.h2o.node.log.dir", new File(project.getBuildDir(), "h2ologs-stest/nodes")
    systemProperty "spark.ext.h2o.client.log.dir", new File(project.getBuildDir(), "h2ologs-stest/client")

    // Set sparkling water assembly jar location
    systemProperty "sparkling.assembly.jar", "${configurations.sparklingWaterAssemblyJar.singleFile}"
    // Run with assertions ON
    enableAssertions = true
    // For a new JVM for each test class
    forkEvery = 1

    // Increase test runner memory
    maxHeapSize = "4g"
    // Working dir will be root project
    workingDir = rootDir

    // Show standard out and standard error of the test JVM(s) on the console
    testLogging.showStandardStreams = true
}

integTest.dependsOn checkSparkVersionTask
scriptsTest.dependsOn checkSparkVersionTask

defineStandardPublication().call()
