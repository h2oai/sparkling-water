description = "Sparkling Water Examples"

apply from: "$rootDir/gradle/utils.gradle"
apply from: "$rootDir/gradle/scriptsTest.gradle"

project.ext.exampleResources = project.file("flows")

sourceSets {
    main {
        resources {
            srcDir("flows")
        }
    }
}

jar {
    eachFile { f ->
        if (new File(exampleResources, f.file.name).exists()) {
            f.path = "www/flow/packs/examples/$f.name"
        }
    }
}

configurations {
    sparklingWaterAssemblyJar
}

dependencies {

    sparklingWaterAssemblyJar project(path: ':sparkling-water-assembly', configuration: 'shadow')
    compile project(path: ':sparkling-water-assembly', configuration: 'shadow')

    // H2O HDFS Persistent layer
    compile("ai.h2o:h2o-persist-hdfs:${h2oVersion}") {
        // Cannot use here: transitive = false since Gradle is producing wrong POM file
        // Hence the exclusions are listed manually
        exclude(group: "org.apache.hadoop", module: "hadoop-client")
        exclude(group: "org.apache.hadoop", module: "hadoop-hdfs-client")
        exclude(group: "org.apache.hadoop", module: "hadoop-aws")
    }


    // And Scala library
    compile "org.scala-lang:scala-library:${scalaVersion}"

    // Add streaming
    compileOnly "org.apache.spark:spark-streaming_${scalaBaseVersion}:${sparkVersion}"
    testCompile "org.apache.spark:spark-streaming_${scalaBaseVersion}:${sparkVersion}"

    testCompile "org.apache.spark:spark-repl_${scalaBaseVersion}:${sparkVersion}"

    // And use scalatest for Scala testing
    testCompile "org.scalatest:scalatest_${scalaBaseVersion}:2.2.1"
    testCompile "junit:junit:4.11"
    // to reuse some utils test classes defined in the core
    testCompile project(path: ':sparkling-water-core', configuration: 'testArchives')

    // Integration tests requirements
    integTestCompile "org.scalatest:scalatest_${scalaBaseVersion}:2.2.1"
    integTestCompile "junit:junit:4.11"
    // to reuse some utils test classes defined in the core
    integTestCompile project(path: ':sparkling-water-core', configuration: 'testArchives')

    // To enable Idea compiler even for integTestCode which use mllib
    compile "org.apache.spark:spark-mllib_${scalaBaseVersion}:${sparkVersion}"

    // Put Spark Assembly on runtime path
    integTestRuntime fileTree(dir: new File((String) sparkHome, "lib/"), include: '*.jar')
}

integTest {
    systemProperty "spark.ext.h2o.backend.cluster.mode", detectBackendClusterMode()
    systemProperty "spark.testing", "true"
    systemProperty "spark.test.home", "${sparkHome}"
    systemProperty "sparkling.test.hdp.version", "${hdpVersion}"

    // Pass list of jars required for testing
    systemProperty "sparkling.assembly.jar", "${configurations.sparklingWaterAssemblyJar.singleFile}"
    systemProperty "sparkling.itest.jar", "${integTestJar.archivePath}"
    systemProperty "spark.driver.extraJavaOptions", "-Dhdp.version=${hdpVersion} ${extraJVMArgs}"
    systemProperty "spark.yarn.am.extraJavaOptions", "-Dhdp.version=${hdpVersion} ${extraJVMArgs}"
    systemProperty "spark.executor.extraJavaOptions", "-Dhdp.version=${hdpVersion} ${extraJVMArgs}"

    // Show standard out and standard error of the test JVM(s) on the console
    testLogging.showStandardStreams = true
}

// Setup test environment for scripts test
scriptsTest {
    systemProperty "spark.ext.h2o.backend.cluster.mode", detectBackendClusterMode()
    systemProperty "spark.testing", "true"
    systemProperty "spark.test.home", "${sparkHome}"
    systemProperty "sparkling.test.log.dir", new File(project.getBuildDir(), "h2ologs-test")
    systemProperty "spark.ext.h2o.node.log.dir", new File(project.getBuildDir(), "h2ologs-stest/nodes")
    systemProperty "spark.ext.h2o.client.log.dir", new File(project.getBuildDir(), "h2ologs-stest/client")
    systemProperty "spark.driver.extraJavaOptions", "${extraJVMArgs}"
    systemProperty "spark.executor.extraJavaOptions", "${extraJVMArgs}"

    // Set sparkling water assembly jar location
    systemProperty "sparkling.assembly.jar", "${configurations.sparklingWaterAssemblyJar.singleFile}"
    // Run with assertions ON
    enableAssertions = true
    // For a new JVM for each test class
    forkEvery = 1

    if (!jvmArgs.isEmpty()) {
        jvmArgs extraJVMArgs.split(" ")
    }
    // Increase test runner memory
    maxHeapSize = "4g"
    // Working dir will be root project
    workingDir = rootDir

    // Show standard out and standard error of the test JVM(s) on the console
    testLogging.showStandardStreams = true
}

integTest.dependsOn checkSparkVersionTask
scriptsTest.dependsOn checkSparkVersionTask

defineStandardPublication().call()
