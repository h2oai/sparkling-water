apply from: "$rootDir/gradle/utils.gradle"

/* Get spark home value from local environment */
String getSparkHome() {
    String sh =
        [ project.hasProperty("sparkHome") ? project["sparkHome"] : null,
          System.properties["SPARK_HOME"],
          System.env['SPARK_HOME']
        ].find { h -> h!=null ? new File(h).isDirectory() : false }
    if (sh!=null) {
        logger.info("Found spark home: ${sh}")
        return sh
    } else {
        return null
    }
}

/* Get spark home value from local environment */
String getSparkMaster() {
    String sh =
            [ project.hasProperty("sparkMaster") ? project["sparkMaster"] : null,
              System.properties["MASTER"],
              System.env['MASTER']
            ].find { h -> h!=null }
    if (sh!=null) {
        logger.info("Found spark master: ${sh}")
        return sh
    } else {
        return null
    }
}
String getSparkSubmitScriptName(){
    if(isWindowsBased()){
        return "spark-submit.cmd"
    }else{
        return "spark-submit"
    }
}
String getSparkSubmitScriptPath(String sparkHome){
    new File(new File((String) sparkHome), "bin" + File.separator + getSparkSubmitScriptName()).absolutePath
}

String getSystemSparkVersion()  {
   String sparkHome = getSparkHome()
   if (sparkHome == null)
       throw new InvalidUserDataException("Cannot find Spark home - " +
                                          "please specify SPARK_HOME env variable or pass -PsparkHome property to gradle script")
   // We need to pass current Spark home to script which determines the Spark version
   // This is needed in cases when we specify sparkHome using -P in gradle
   // In that case gradle SPARK_HOME is set to specified Spark location but global SPARK_HOME is unchanged
   // and we need to temporary change it in child process to get the correct Spark version
   def commandEnvVars
   if(isWindowsBased()){
    // In case of Windows we can't set environment variables in execute command as it breaks the build
    commandEnvVars = null
   }else{
    commandEnvVars = ["SPARK_HOME="+getSparkHome()]
   }

   def command
   if(isWindowsBased()){
       // In case of Windows we can't set environment variable in execute command so we set it them as part of the command
       command = getOsSpecificCommandLine(["SET SPARK_HOME=${getSparkHome()} &", getSparkSubmitScriptPath(sparkHome), "--version DUMMY_TEXT_FOR_SPARK20_PREVIEW"]).join(" ")
   }else{
       command = getOsSpecificCommandLine([getSparkSubmitScriptPath(sparkHome), "--version DUMMY_TEXT_FOR_SPARK20_PREVIEW"]).join(" ")
   }
   def commandOutput = command.execute(commandEnvVars,null).err.text
   def response = commandOutput.readLines()
   def version_line = response.find{ it.contains("version")}
   if (version_line == null) throw new InvalidUserDataException("Failed to detect Spark version, command was " + command + ", response was\n" + response)
   return version_line.replaceAll(".*version ","")
}


String getHdpVersion() {
  String sh =
            [ project.hasProperty("hdpVersion") ? project["hdpVersion"] : null,
              System.properties['HDP_VERSION'],
              System.env['HDP_VERSION']
            ].find { h -> h!=null }
  return sh
}

void checkSparkVersion() {
    String systemSparkVersion = getSystemSparkVersion()
    String majorSparkVersion = systemSparkVersion.count(".") == 1 ? systemSparkVersion : systemSparkVersion.substring(0, systemSparkVersion.lastIndexOf('.'))
    if (!ext.sparkVersion.startsWith(majorSparkVersion)){
        throw new InvalidUserDataException("You are trying to build Sparkling Water for Spark $ext.sparkVersion, but your \$SPARK_HOME(=${getSparkHome()}) property points to Spark of version ${getSystemSparkVersion()}. Please ensure correct Spark is provided and re-run Sparkling Water.")
    }
}

// Useful task to setup idea test configuration
task printTestEnv << {
    println("-Dspark.testing=true\n")
}
task printIntegTestEnv << {
    println("-Dspark.testing=true\n" +
            "-Dsparkling.assembly.jar=${project(":sparkling-water-assembly").configurations.shadow.artifacts.file.join(',')}\n" +
            "-Dsparkling.itest.jar=${integTestJar.archivePath}\n"+
            "-Dsparkling.pysparkling.egg=${project(":sparkling-water-py").configurations.eggs.artifacts.file.join(',')}\n"+
            "-Dsparkling.test.hdp.version=current\n"+
            "-DSPARK_HOME=${sparkHome}")
}

task printScriptsTestEnv << {
    println("-Dsparkling.assembly.jar=${project(":sparkling-water-assembly").configurations.shadow.artifacts.file.join(',')}\n")
}

ext.sparkHome = getSparkHome()
ext.sparkMaster = getSparkMaster()
ext.hdpVersion = getHdpVersion()
ext.checkSparkVersion = this.&checkSparkVersion

// Run the check but not at configuration time!
// It needs to run at execution time of build file.

task checkSparkVersionTask << {
    checkSparkVersion()
}

afterEvaluate { project ->
    Task task = project.tasks.findByName("check")
    if (task != null) {
        task.dependsOn checkSparkVersionTask
    }
}
