#!/usr/bin/groovy
@Library('test-shared-library') _

IMAGE_VERSION = 1
CLUSTER_MODE = "internal"

properties(
        [
                pipelineTriggers([cron('H 21 * * *')]),
                buildDiscarder(logRotator(numToKeepStr: '10'))
        ]
)

// Only PR builds have the CHANGE_BRANCH set
if (env.CHANGE_BRANCH != null && env.CHANGE_BRANCH != ''){
    cancelPreviousBuilds()
}

node('docker && micro') {
    docker.withRegistry("http://harbor.h2o.ai") {

        // Clean workspace
        cleanWs()

        // Get Sparkling Water and save the scm environment variables
        checkout scm

        def customEnv = [
                "SPARK=spark-2.3.1-bin-hadoop2.7",
                "SPARK_HOME=${env.WORKSPACE}/spark",
                "HADOOP_CONF_DIR=/etc/hadoop/conf",
                "MASTER=yarn-client",
                "H2O_PYTHON_WHEEL=${env.WORKSPACE}/private/h2o.whl",
        ]

        ansiColor('xterm') {
            timestamps {
                withEnv(customEnv) {
                    timeout(time: 180, unit: 'MINUTES') {
                        dir("${env.WORKSPACE}") {
                            prepareSparkEnvironment()()
                            prepareSparklingWaterEnvironment()()
                            buildAndLint()()
                            smokeTestsHDP22()()
                            smokeTestsHDP23()()
                            smokeTestsHDP24()()
                            smokeTestsHDP25()()
                            smokeTestsHDP26()()
                            smokeTestsCDH54()()
                            smokeTestsCDH55()()
                            smokeTestsCDH56()()
                            smokeTestsCDH57()()
                            smokeTestsCDH58()()
                            smokeTestsCDH510()()
                            smokeTestsCDH513()()
                            smokeTestsCDH514()()
                        }
                    }
                }
            }
        }

    }
}


def testInsideDocker(hadoopVersion) {
    def image = 'opsh2oai/sparkling-water-hadoop-' + hadoopVersion + ":" + IMAGE_VERSION
    retryWithDelay(3, 120, {
        withCredentials([usernamePassword(credentialsId: "harbor.h2o.ai", usernameVariable: 'REGISTRY_USERNAME', passwordVariable: 'REGISTRY_PASSWORD')]) {
            sh "docker login -u $REGISTRY_USERNAME -p $REGISTRY_PASSWORD harbor.h2o.ai"
            sh "docker pull harbor.h2o.ai/${image}"
        }
    })
    docker.image(image).inside("--init --dns 172.16.0.200 -v /home/0xdiag:/home/0xdiag --privileged") {
        sh "activate_java_8"
        try {
            sh """
                     sudo -E /usr/sbin/startup.sh
                     
                     hdfs dfs -put /home/0xdiag/smalldata/parser/orc/prostate_NA.orc
                     hdfs dfs -put /home/0xdiag/smalldata/parser/parquet/airlines-simple.snappy.parquet
                     hdfs dfs -put /home/0xdiag/smalldata/junit/iris.xls      
                     . /envs/h2o_env_python2.7/bin/activate
                     PYSPARK_DRIVER_PYTHON=\$(which python) PYSPARK_PYTHON=\$(which python) ./gradlew build -x check hadoopSmokeTests -PbackendMode=internal -PsparkMaster=${env.MASTER} -PsparkHome=${env.SPARK_HOME} -x check
                     # echo 'Archiving artifacts smoke tests for Hadoop'
                    """
        } finally {
            arch '**/build/*tests.log,**/*.log, **/out.*, **/*py.out.txt, **/stdout, **/stderr,**/build/**/*log*, py/build/py_*_report.txt, **/build/reports/'
        }
    }
}

def retryWithDelay(final int retries, final int delay, final Closure body) {
    for (def i = 0; i < retries; i++) {
        try {
            body()
            break
        } catch (Exception e) {
            if (i == (retries - 1)) {
                throw e
            }
            script.sleep(delay)
        }
    }
}

def withDocker(code) {
    def image = 'opsh2oai/sparkling_water_tests:7'
    retryWithDelay(3, 120, {
        withCredentials([usernamePassword(credentialsId: "harbor.h2o.ai", usernameVariable: 'REGISTRY_USERNAME', passwordVariable: 'REGISTRY_PASSWORD')]) {
            sh "docker login -u $REGISTRY_USERNAME -p $REGISTRY_PASSWORD harbor.h2o.ai"
            sh "docker pull harbor.h2o.ai/${image}"
        }
    })
    docker.image(image).inside("--init --dns 172.16.0.200 -v /home/0xdiag:/home/0xdiag --privileged") {
        sh "activate_java_8"
        code()
    }
}



def prepareSparkEnvironment() {
    return { ->
        stage('Prepare Spark Environment - ' + CLUSTER_MODE) {
            withDocker() {
                withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', credentialsId: 'AWS S3 Credentials', accessKeyVariable: 'AWS_ACCESS_KEY_ID', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {

                    sh """
                    cp -R \${SPARK_HOME_2_4_0} ${env.SPARK_HOME}

echo "spark.hadoop.fs.s3a.impl                org.apache.hadoop.fs.s3a.S3AFileSystem" >> ${env.SPARK_HOME}/conf/spark-defaults.conf
echo "spark.hadoop.fs.s3a.access.key          ${AWS_ACCESS_KEY_ID}" >> ${env.SPARK_HOME}/conf/spark-defaults.conf
echo "spark.hadoop.fs.s3a.secret.key          ${AWS_SECRET_ACCESS_KEY}" >> ${env.SPARK_HOME}/conf/spark-defaults.conf
echo "spark.hadoop.fs.s3n.impl                org.apache.hadoop.fs.s3native.NativeS3FileSystem" >> ${env.SPARK_HOME}/conf/spark-defaults.conf
echo "spark.hadoop.fs.s3n.awsAccessKeyId      ${AWS_ACCESS_KEY_ID}" >> ${env.SPARK_HOME}/conf/spark-defaults.conf
echo "spark.hadoop.fs.s3n.awsSecretAccessKey  ${AWS_SECRET_ACCESS_KEY}" >> ${env.SPARK_HOME}/conf/spark-defaults.conf
                    """
                }
            }
        }
    }
}

def prepareSparklingWaterEnvironment() {
    return { ->
        stage('QA: Prepare Sparkling Water Environment - ' + CLUSTER_MODE) {
            withDocker() {
                // Warm up Gradle wrapper. When the gradle wrapper is downloaded for the first time, it prints message
                // with release notes which can mess up the build
                sh """
                ${env.WORKSPACE}/gradlew --help
                """

                sh """
                    # Download h2o-python client, save it in private directory
                    # and export variable H2O_PYTHON_WHEEL driving building of pysparkling package
                    mkdir -p ${env.WORKSPACE}/private/
                    curl -s `${env.WORKSPACE}/gradlew -Dorg.gradle.internal.launcher.welcomeMessageEnabled=false -q printH2OWheelPackage` > ${env.WORKSPACE}/private/h2o.whl
                """
            }
        }
    }
}

def buildAndLint() {
    return { ->
        stage('QA: Build and Lint - ' + CLUSTER_MODE) {
            withDocker() {
                withCredentials([usernamePassword(credentialsId: "LOCAL_NEXUS", usernameVariable: 'LOCAL_NEXUS_USERNAME', passwordVariable: 'LOCAL_NEXUS_PASSWORD')]) {
                    sh """
                    # Build
                    ${env.WORKSPACE}/gradlew clean build -x check scalaStyle -PlocalNexusUsername=$LOCAL_NEXUS_USERNAME -PlocalNexusPassword=$LOCAL_NEXUS_PASSWORD
                    """
                }
            }
        }
    }
}

def smokeTestsHDP22() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 HDP 2.2 - ' + CLUSTER_MODE) {
            testInsideDocker("hdp-2.2")
        }
    }
}

def smokeTestsHDP23() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 HDP 2.3 - ' + CLUSTER_MODE) {
            testInsideDocker("hdp-2.3")
        }
    }
}

def smokeTestsHDP24() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 HDP 2.4 - ' + CLUSTER_MODE) {
            testInsideDocker("hdp-2.4")
        }
    }
}

def smokeTestsHDP25() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 HDP 2.5 - ' + CLUSTER_MODE) {
            testInsideDocker("hdp-2.5")
        }
    }
}

def smokeTestsHDP26() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 HDP 2.6 - ' + CLUSTER_MODE) {
            testInsideDocker("hdp-2.6")
        }
    }
}

def smokeTestsCDH54() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 CDH 5.4 - ' + CLUSTER_MODE) {
            testInsideDocker("cdh-5.4")
        }
    }
}

def smokeTestsCDH55() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 CDH 5.5 - ' + CLUSTER_MODE) {
            testInsideDocker("cdh-5.5")
        }
    }
}

def smokeTestsCDH56() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 CDH 5.6 - ' + CLUSTER_MODE) {
            testInsideDocker("cdh-5.6")
        }
    }
}

def smokeTestsCDH57() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 CDH 5.7 - ' + CLUSTER_MODE) {
            testInsideDocker("cdh-5.7")
        }
    }
}

def smokeTestsCDH58() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 CDH 5.8 - ' + CLUSTER_MODE) {
            testInsideDocker("cdh-5.8")
        }
    }
}

def smokeTestsCDH510() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 CDH 5.10 - ' + CLUSTER_MODE) {
            testInsideDocker("cdh-5.10")
        }
    }
}

def smokeTestsCDH513() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 CDH 5.13 - ' + CLUSTER_MODE) {
            testInsideDocker("cdh-5.13")
        }
    }
}

def smokeTestsCDH514() {
    return { ->
        stage('Hadoop Smoke Tests 2.7 CDH 5.14 - ' + CLUSTER_MODE) {
            testInsideDocker("cdh-5.14")
        }
    }
}