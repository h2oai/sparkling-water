#!/usr/bin/groovy
@Library('test-shared-library') _

// Job parameters
properties(
        [
                parameters(
                        [
                                booleanParam(name: 'runUnitTests', defaultValue: true, description: 'Run unit tests'),
                                booleanParam(name: 'runPyUnitTests', defaultValue: true, description: 'Run PyUnit tests'),
                                booleanParam(name: 'runRUnitTests', defaultValue: true, description: 'Run PyRUnit tests'),
                                booleanParam(name: 'runLocalIntegTests', defaultValue: true, description: 'Run local integration tests'),
                                booleanParam(name: 'runLocalPyIntegTests', defaultValue: true, description: 'Run local Python integration tests'),
                                booleanParam(name: 'runScriptTests', defaultValue: true, description: 'Run script tests'),
                                booleanParam(name: 'runIntegTests', defaultValue: false, description: 'Run integration tests'),
                                booleanParam(name: 'runPySparklingIntegTests', defaultValue: true, description: 'Run pySparkling integration tests'),
                                choice(
                                        choices: 'hdp2.2\nhdp2.3\nhdp2.4\nhdp2.5\nhdp2.6\ncdh5.4\ncdh5.5\ncdh5.6\ncdh5.7\ncdh5.8\ncdh5.10\nmapr4.0\nmapr5.0\nmapr5.1\nmapr5.2\niop4.2',
                                        description: 'Hadoop version for which H2O driver is obtained, used only in external mode',
                                        name: 'driverHadoopVersion'),
                                string(name: 'hdpVersion', defaultValue: '2.2.6.3-1', description: 'HDP version to pass to Spark configuration - for example, 2.2.0.0-2041, or 2.6.0.2.2, or current. When running external tests on yarn, the current will not do since it is not automatically expanded -> so please set 2.2.6.3-1'),
                                choice(
                                        choices: 'both\ninternal\nexternal',
                                        description: "Choose in which mode to run tests",
                                        name: 'backendMode'
                                )
                        ]
                ),
                buildDiscarder(logRotator(numToKeepStr: '10'))
        ]
)

cancelPreviousBuilds()

def pipeline
def sparkVersions
node("master") {
    cleanWs()
    checkout scm
    pipeline = load 'jenkins/sparklingWaterPipeline.groovy'
    def versionLine = readFile("gradle.properties")
            .split("\n").find() { line -> line.startsWith('supportedSparkVersions') }
    sparkVersions = versionLine.split("=")[1].split(" ")

    if (isPrJob()) {
        // test PRs only in first and last supported Spark
        sparkVersions = [sparkVersions.first(), sparkVersions.last()]
        throw new RuntimeException("Running on $sparkVersions")
    }
}


pipeline(params) { p ->
    sparkMajorVersions = sparkVersions
    runUnitTests = "${p.runUnitTests}"
    runPyUnitTests = "${p.runPyUnitTests}"
    runRUnitTests = "${p.runRUnitTests}"
    runLocalIntegTests = "${p.runLocalIntegTests}"
    runLocalPyIntegTests = "${p.runLocalPyIntegTests}"
    runScriptTests = "${p.runScriptTests}"
    runIntegTests = "${p.runIntegTests}"
    runPySparklingIntegTests = "${p.runPySparklingIntegTests}"
    buildAgainstH2OBranch = "true"
    h2oBranch = "master"
    hadoopVersion = "2.7"
    hdpVersion = "${p.hdpVersion}"
    driverHadoopVersion = "${p.driverHadoopVersion}"
    uploadNightly = "false"
    backendMode = "${p.backendMode}"
}
