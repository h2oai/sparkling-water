#!/usr/bin/groovy
@Library('test-shared-library') _

properties(
        [
                pipelineTriggers([cron('H 23 * * *')]),
                buildDiscarder(logRotator(numToKeepStr: '10'))
        ]
)

node('docker && micro') {
    docker.withRegistry("https://docker.h2o.ai") {

        // Clean workspace
        sh 'rm -rf *'
        // Get Sparkling Water and save the scm environment variables
        checkout scm
        // Get the script with the pipeline
        def pipeline = load 'jenkins/sparklingWaterPipeline.groovy'

        // Execute the pipeline
        pipeline(params) { p ->
            sparkVersion = "2.3.1"
            runUnitTests = "true"
            runPyUnitTests = "true"
            runLocalIntegTests = "true"
            runLocalPyIntegTests = "true"
            runScriptTests = "true"
            runIntegTests = "false"
            runPySparklingIntegTests = "true"
            sparklingTestEnv = "yarn"
            buildAgainstH2OBranch = "false"
            h2oBranch = "master"
            buildAgainstSparkBranch = "false"
            sparkBranch = "master"
            hadoopVersion = "2.7"
            backendMode = "external"
            hdpVersion = "2.2.6.3-1"
            driverHadoopVersion = "hdp2.2"
            buildNightly = "true"
            // We have 2 nightly jobs - for external and internal cluster for testing purposes,
            // however it is sufficient to create nightly just after one of these jobs finish
            // We therefore don't publish nightly here, but in the internal nightly job.
            // Note: We can do that as these 2 jobs run at the same time
            uploadNightly = "false"
            dockerVersion = "3"

        }
    }
}
