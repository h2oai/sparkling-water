{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.2.17:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x111c58810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that SparkSession is available\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://172.16.2.17:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>07 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.16.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>6 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>sparkling-water-kuba_local-1512088902004</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.416 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://172.16.2.17:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.14 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         07 secs\n",
       "H2O cluster version:        3.16.0.1\n",
       "H2O cluster version age:    6 days\n",
       "H2O cluster name:           sparkling-water-kuba_local-1512088902004\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.416 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://172.16.2.17:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             2.7.14 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * H2O name: sparkling-water-kuba_local-1512088902004\n",
      " * cluster size: 1\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (driver,172.16.2.17,54321)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://172.16.2.17:54321 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Start H2OContext in internal backend\n",
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Load \n",
    "import h2o\n",
    "reviews_h2o = h2o.upload_file(\"Reviews.csv\")\n",
    "reviews_spark = hc.as_spark_frame(reviews_h2o)\n",
    "schema = reviews_spark.schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Score=5, Time=1303862400, Summary=u'Good Quality Dog Food'),\n",
       " Row(Score=1, Time=1346976000, Summary=u'Not as Advertised'),\n",
       " Row(Score=4, Time=1219017600, Summary=u'\"\"Delight\"\" says it a'),\n",
       " Row(Score=2, Time=1307923200, Summary=u'Cough Medicine'),\n",
       " Row(Score=5, Time=1350777600, Summary=u'Great taffy'),\n",
       " Row(Score=4, Time=1342051200, Summary=u'Nice Taffy'),\n",
       " Row(Score=5, Time=1340150400, Summary=u'Great!  Just as good as the expensive brands!'),\n",
       " Row(Score=5, Time=1336003200, Summary=u'Wonderful, tasty taffy'),\n",
       " Row(Score=5, Time=1322006400, Summary=u'Yay Barley'),\n",
       " Row(Score=5, Time=1351209600, Summary=u'Healthy Dog Food')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "colSelect = SQLTransformer(\n",
    "    statement=\"SELECT Score, Time, Summary FROM __THIS__\")\n",
    "\n",
    "# Show slice of transformed data\n",
    "selected = colSelect.transform(reviews_spark)\n",
    "selected.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(from_unixtime(CAST(Time AS BIGINT), yyyy-MM-dd HH:mm:ss)=u'2011-04-26 17:00:00')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refineTime = SQLTransformer(\n",
    "    statement=\"SELECT dayofweek(from_unixtime(Time) FROM __THIS__\")\n",
    "\n",
    "refined = refineTime.transform(selected)\n",
    "refined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = false)\n",
      " |-- ProductId: string (nullable = false)\n",
      " |-- UserId: string (nullable = false)\n",
      " |-- ProfileName: string (nullable = false)\n",
      " |-- HelpfulnessNumerator: short (nullable = false)\n",
      " |-- HelpfulnessDenominator: short (nullable = false)\n",
      " |-- Score: byte (nullable = false)\n",
      " |-- Time: integer (nullable = false)\n",
      " |-- Summary: string (nullable = false)\n",
      " |-- Text: string (nullable = false)\n",
      " |-- Day: byte (nullable = false)\n",
      " |-- Month: byte (nullable = false)\n",
      " |-- Year: short (nullable = false)\n",
      " |-- WeekNum: byte (nullable = false)\n",
      " |-- WeekDay: string (nullable = false)\n",
      " |-- HourOfDay: byte (nullable = false)\n",
      " |-- Weekend: byte (nullable = false)\n",
      " |-- Season: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert data to spark Data Frame\n",
    "df = hc.as_spark_frame(reviews)\n",
    "# Show the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# Calculate average score per Year\n",
    "avg_score_per_year = hc.as_h2o_frame(df.groupBy(\"Year\").agg(mean(\"Score\"), count(\"Score\")), \"avgScorePerYear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate average score per Month\n",
    "avg_score_per_month = hc.as_h2o_frame(df.groupBy(\"Month\").agg(mean(\"Score\"), count(\"Score\")), \"avgScorePerMonth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate average score per Day\n",
    "avg_score_per_month = hc.as_h2o_frame(df.groupBy(\"WeekDay\").agg(mean(\"Score\"), count(\"Score\")), \"avgScorePerDay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate input data frame for sentiment analysis\n",
    "sentimentDF = hc.as_spark_frame(reviews[[\"Score\", \"Month\", \"Day\", \"WeekDay\", \"Summary\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, IDF, HashingTF\n",
    "\n",
    "\n",
    "# Prepare data for the pipeline\n",
    "toFloat = udf(lambda score: float(score), FloatType())\n",
    "sentimentDF.where(\"Score != 3\").withColumn(\"Score\", toFloat(col(\"Score\")))\n",
    "\n",
    "# Define the pipeline stages\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Summary\",\n",
    "                                outputCol=\"TokenizedSummary\",\n",
    "                                pattern=\"[, ]\",\n",
    "                                toLowercase=True)\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(),\n",
    "                                    outputCol=\"CleanedSummary\",\n",
    "                                    caseSensitive=False)\n",
    "## Hash the words\n",
    "hashingTF = HashingTF(inputCol=stopWordsRemover.getOutputCol(),\n",
    "                      outputCol=\"wordToIndex\",\n",
    "                      numFeatures=1 << 10)\n",
    "\n",
    "\n",
    "## Create inverse document frequencies model\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(),\n",
    "          outputCol=\"tf_idf\",\n",
    "          minDocFreq=1)\n",
    "\n",
    "from pysparkling.ml import ColumnPruner, H2OGBM\n",
    "\n",
    "## Create GBM model\n",
    "gbm = H2OGBM(ratio=0.8,\n",
    "             featuresCols=[idf.getOutputCol()],\n",
    "             predictionCol=\"Summary\")\n",
    "\n",
    "## Remove all helper columns\n",
    "colPruner = ColumnPruner(columns=[idf.getOutputCol(), hashingTF.getOutputCol(), stopWordsRemover.getOutputCol(), regexTokenizer.getOutputCol()])\n",
    "\n",
    "##  Create the pipeline by defining all the stages\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopWordsRemover, hashingTF, idf, gbm, colPruner])\n",
    "\n",
    "## Train the pipeline model\n",
    "model = pipeline.fit(sentimentDF)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Score=5, Month=1, Day=16, WeekDay=u'Fri', Summary=u'Good Quality Dog Food', value=4.054493898772716),\n",
       " Row(Score=1, Month=1, Day=16, WeekDay=u'Fri', Summary=u'Not as Advertised', value=4.054493898772716),\n",
       " Row(Score=4, Month=1, Day=15, WeekDay=u'Thu', Summary=u'\"\"Delight\"\" says it a', value=4.054493898772716),\n",
       " Row(Score=2, Month=1, Day=16, WeekDay=u'Fri', Summary=u'Cough Medicine', value=4.054493898772716),\n",
       " Row(Score=5, Month=1, Day=16, WeekDay=u'Fri', Summary=u'Great taffy', value=4.613358328249387),\n",
       " Row(Score=4, Month=1, Day=16, WeekDay=u'Fri', Summary=u'Nice Taffy', value=4.054493898772716),\n",
       " Row(Score=5, Month=1, Day=16, WeekDay=u'Fri', Summary=u'Great!  Just as good as the expensive brands!', value=4.18077098095041),\n",
       " Row(Score=5, Month=1, Day=16, WeekDay=u'Fri', Summary=u'Wonderful, tasty taffy', value=4.502975996597751),\n",
       " Row(Score=5, Month=1, Day=16, WeekDay=u'Fri', Summary=u'Yay Barley', value=4.054493898772716),\n",
       " Row(Score=5, Month=1, Day=16, WeekDay=u'Fri', Summary=u'Healthy Dog Food', value=4.199643722982227)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(sentimentDF).take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Score=5, Month=1, Day=16, WeekDay=u'Fri', Summary=u'Good Quality Dog Food')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentDF.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
