{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.2.17:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10656d810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that SparkSession is available\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://172.16.2.17:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>07 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.16.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>6 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>sparkling-water-kuba_local-1512095833528</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.416 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://172.16.2.17:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.14 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         07 secs\n",
       "H2O cluster version:        3.16.0.1\n",
       "H2O cluster version age:    6 days\n",
       "H2O cluster name:           sparkling-water-kuba_local-1512095833528\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.416 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://172.16.2.17:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             2.7.14 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * H2O name: sparkling-water-kuba_local-1512095833528\n",
      " * cluster size: 1\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (driver,172.16.2.17,54321)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://172.16.2.17:54321 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Start H2OContext in internal backend\n",
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |███████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Load data using H2O because we belive in our CSV parser correctly identifies the schema\n",
    "import h2o\n",
    "reviews_h2o = h2o.upload_file(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to Spark frame for the input to the pipeline\n",
    "reviews_spark = hc.as_spark_frame(reviews_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = false)\n",
      " |-- ProductId: string (nullable = false)\n",
      " |-- UserId: string (nullable = false)\n",
      " |-- ProfileName: string (nullable = false)\n",
      " |-- HelpfulnessNumerator: short (nullable = false)\n",
      " |-- HelpfulnessDenominator: short (nullable = false)\n",
      " |-- Score: byte (nullable = false)\n",
      " |-- Time: integer (nullable = false)\n",
      " |-- Summary: string (nullable = false)\n",
      " |-- Text: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the origianl spark schema. We will reuse this schema during online streaming for each row on the input\n",
    "reviews_spark.printSchema()\n",
    "\n",
    "f = open('schema.txt','w')\n",
    "f.write(str(reviews_spark.schema))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Score=5, Time=u'2011-04-26 17:00:00', Summary=u'Good Quality Dog Food'),\n",
       " Row(Score=1, Time=u'2012-09-06 17:00:00', Summary=u'Not as Advertised'),\n",
       " Row(Score=4, Time=u'2008-08-17 17:00:00', Summary=u'\"\"Delight\"\" says it a'),\n",
       " Row(Score=2, Time=u'2011-06-12 17:00:00', Summary=u'Cough Medicine'),\n",
       " Row(Score=5, Time=u'2012-10-20 17:00:00', Summary=u'Great taffy'),\n",
       " Row(Score=4, Time=u'2012-07-11 17:00:00', Summary=u'Nice Taffy'),\n",
       " Row(Score=5, Time=u'2012-06-19 17:00:00', Summary=u'Great!  Just as good as the expensive brands!'),\n",
       " Row(Score=5, Time=u'2012-05-02 17:00:00', Summary=u'Wonderful, tasty taffy'),\n",
       " Row(Score=5, Time=u'2011-11-22 16:00:00', Summary=u'Yay Barley'),\n",
       " Row(Score=5, Time=u'2012-10-25 17:00:00', Summary=u'Healthy Dog Food')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we define all the stages for the pipeline\n",
    "\n",
    "# Drop unnecessary columns\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "colSelect = SQLTransformer(\n",
    "    statement=\"SELECT Score, from_unixtime(Time) as Time, Summary FROM __THIS__\")\n",
    "\n",
    "# Show slice of transformed data\n",
    "selected = colSelect.transform(reviews_spark)\n",
    "selected.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Score=5, Summary=u'Good Quality Dog Food', Day=26, Month=4, Year=2011, WeekNum=17, WeekDay=u'Tue', HourOfDay=17, Weekend=0, Seasson=u'Spring')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create More human readable date columns\n",
    "refineTime = SQLTransformer(\n",
    "    statement=\"SELECT Score, Summary, dayofmonth(Time) as Day, month(Time) as Month, year(Time) as Year, weekofyear(Time) as WeekNum, date_format(Time, 'EEE') as WeekDay, hour(Time) as HourOfDay, IF(date_format(Time, 'EEE')='Sat' OR date_format(Time, 'EEE')='Sun', 1, 0) as Weekend, CASE WHEN month(TIME)=12 OR month(Time)<=2 THEN 'Winter' WHEN month(TIME)>=3 OR month(Time)<=5 THEN 'Spring' WHEN month(TIME)>=6 AND month(Time)<=9 THEN 'Summer' ELSE 'Autumn' END as Seasson FROM __THIS__\")\n",
    "\n",
    "refined = refineTime.transform(selected)\n",
    "refined.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Score: byte (nullable = false)\n",
      " |-- Summary: string (nullable = false)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- WeekNum: integer (nullable = true)\n",
      " |-- WeekDay: string (nullable = true)\n",
      " |-- HourOfDay: integer (nullable = true)\n",
      " |-- Weekend: integer (nullable = false)\n",
      " |-- Seasson: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema\n",
    "refined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Sentiment=u'POSITIVE', Summary=u'Good Quality Dog Food', Day=26, Month=4, Year=2011, WeekNum=17, WeekDay=u'Tue', HourOfDay=17, Weekend=0, Seasson=u'Spring'),\n",
       " Row(Sentiment=u'NEGATIVE', Summary=u'Not as Advertised', Day=6, Month=9, Year=2012, WeekNum=36, WeekDay=u'Thu', HourOfDay=17, Weekend=0, Seasson=u'Spring'),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'\"\"Delight\"\" says it a', Day=17, Month=8, Year=2008, WeekNum=33, WeekDay=u'Sun', HourOfDay=17, Weekend=1, Seasson=u'Spring'),\n",
       " Row(Sentiment=u'NEGATIVE', Summary=u'Cough Medicine', Day=12, Month=6, Year=2011, WeekNum=23, WeekDay=u'Sun', HourOfDay=17, Weekend=1, Seasson=u'Spring'),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Great taffy', Day=20, Month=10, Year=2012, WeekNum=42, WeekDay=u'Sat', HourOfDay=17, Weekend=1, Seasson=u'Spring'),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Nice Taffy', Day=11, Month=7, Year=2012, WeekNum=28, WeekDay=u'Wed', HourOfDay=17, Weekend=0, Seasson=u'Spring'),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Great!  Just as good as the expensive brands!', Day=19, Month=6, Year=2012, WeekNum=25, WeekDay=u'Tue', HourOfDay=17, Weekend=0, Seasson=u'Spring'),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Wonderful, tasty taffy', Day=2, Month=5, Year=2012, WeekNum=18, WeekDay=u'Wed', HourOfDay=17, Weekend=0, Seasson=u'Spring'),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Yay Barley', Day=22, Month=11, Year=2011, WeekNum=47, WeekDay=u'Tue', HourOfDay=16, Weekend=0, Seasson=u'Spring'),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Healthy Dog Food', Day=25, Month=10, Year=2012, WeekNum=43, WeekDay=u'Thu', HourOfDay=17, Weekend=0, Seasson=u'Spring')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove neutral reviews and classify the Scores\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, IDF, CountVectorizer\n",
    "\n",
    "filterScore = SQLTransformer(\n",
    "    statement=\"SELECT IF(Score<3,'NEGATIVE', 'POSITIVE') as Sentiment, Summary, Day, Month, Year, WeekNum, WeekDay, HourOfDay, Weekend, Seasson FROM __THIS__ WHERE Score !=3 \")\n",
    "\n",
    "filtered = filterScore.transform(refined)\n",
    "filtered.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the message\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Sentiment\",\n",
    "                                outputCol=\"TokenizedSummary\",\n",
    "                                pattern=\"[, ]\",\n",
    "                                toLowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary words\n",
    "stopWordsRemover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(),\n",
    "                                    outputCol=\"CleanedSummary\",\n",
    "                                    caseSensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hash the words\n",
    "countVectorizer = CountVectorizer(inputCol=stopWordsRemover.getOutputCol(),\n",
    "                      outputCol=\"wordToIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create inverse document frequencies model\n",
    "idf = IDF(inputCol=countVectorizer.getOutputCol(),\n",
    "          outputCol=\"tf_idf\",\n",
    "          minDocFreq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pysparkling.ml import ColumnPruner, H2OGBM\n",
    "\n",
    "# Create GBM model\n",
    "gbm = H2OGBM(ratio=0.8,\n",
    "             featuresCols=[idf.getOutputCol()],\n",
    "             predictionCol=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all helper columns\n",
    "colPruner = ColumnPruner(columns=[idf.getOutputCol(), countVectorizer.getOutputCol(), stopWordsRemover.getOutputCol(), regexTokenizer.getOutputCol()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Create the pipeline by defining all the stages\n",
    "pipeline = Pipeline(stages=[colSelect, refineTime, filterScore, regexTokenizer, stopWordsRemover, countVectorizer, idf, gbm, colPruner])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the pipeline model\n",
    "model = pipeline.fit(reviews_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Sentiment=u'POSITIVE', Summary=u'Good Quality Dog Food', Day=26, Month=4, Year=2011, WeekNum=17, WeekDay=u'Tue', HourOfDay=17, Weekend=0, Seasson=u'Spring', NEGATIVE=0.0010524337424877572, POSITIVE=0.9989475662575122),\n",
       " Row(Sentiment=u'NEGATIVE', Summary=u'Not as Advertised', Day=6, Month=9, Year=2012, WeekNum=36, WeekDay=u'Thu', HourOfDay=17, Weekend=0, Seasson=u'Spring', NEGATIVE=0.994873070841184, POSITIVE=0.0051269291588159115),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'\"\"Delight\"\" says it a', Day=17, Month=8, Year=2008, WeekNum=33, WeekDay=u'Sun', HourOfDay=17, Weekend=1, Seasson=u'Spring', NEGATIVE=0.0010524337424877572, POSITIVE=0.9989475662575122),\n",
       " Row(Sentiment=u'NEGATIVE', Summary=u'Cough Medicine', Day=12, Month=6, Year=2011, WeekNum=23, WeekDay=u'Sun', HourOfDay=17, Weekend=1, Seasson=u'Spring', NEGATIVE=0.994873070841184, POSITIVE=0.0051269291588159115),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Great taffy', Day=20, Month=10, Year=2012, WeekNum=42, WeekDay=u'Sat', HourOfDay=17, Weekend=1, Seasson=u'Spring', NEGATIVE=0.0010524337424877572, POSITIVE=0.9989475662575122),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Nice Taffy', Day=11, Month=7, Year=2012, WeekNum=28, WeekDay=u'Wed', HourOfDay=17, Weekend=0, Seasson=u'Spring', NEGATIVE=0.0010524337424877572, POSITIVE=0.9989475662575122),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Great!  Just as good as the expensive brands!', Day=19, Month=6, Year=2012, WeekNum=25, WeekDay=u'Tue', HourOfDay=17, Weekend=0, Seasson=u'Spring', NEGATIVE=0.0010524337424877572, POSITIVE=0.9989475662575122),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Wonderful, tasty taffy', Day=2, Month=5, Year=2012, WeekNum=18, WeekDay=u'Wed', HourOfDay=17, Weekend=0, Seasson=u'Spring', NEGATIVE=0.0010524337424877572, POSITIVE=0.9989475662575122),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Yay Barley', Day=22, Month=11, Year=2011, WeekNum=47, WeekDay=u'Tue', HourOfDay=16, Weekend=0, Seasson=u'Spring', NEGATIVE=0.0010524337424877572, POSITIVE=0.9989475662575122),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Healthy Dog Food', Day=25, Month=10, Year=2012, WeekNum=43, WeekDay=u'Thu', HourOfDay=17, Weekend=0, Seasson=u'Spring', NEGATIVE=0.0010524337424877572, POSITIVE=0.9989475662575122)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try predicting on the same input data\n",
    "model.transform(reviews_spark).take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the pipeline model\n",
    "model.save(\"Pipeline.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
