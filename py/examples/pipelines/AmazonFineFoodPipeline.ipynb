{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkling Water Pipeline Productionalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Sparkling Water provides access to H2O algorithms and publishes API to integrate them as part of regular Spark pipelines. This feature allow for seamless training and deployment of H2O algorithms in the Spark environment. Futhermore, trained pipelines do not require H2O runtime anymore (thanks to MOJO representation of trained H2O models) which enables variety of deployment scenarios.\n",
    "\n",
    "Moreover, by supporting Python and Scala environment we enable a simple transfer of modeling results between data scientists (Python land) and production (JVM land).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The goal of this hands-on is to:\n",
    "  - show integration of H2O models into Spark pipelines using PySpark and PySparkling\n",
    "  - demonstrate deployment of the trained pipeline in the context of JVM and Spark streaming\n",
    "  \n",
    "Our modeling goal is to predict sentiment of Amazon food reviews. For this purpose, we use a pre-processed dataset from [SNAP repository](https://snap.stanford.edu/data/web-FineFoods.html). The dataset contains multiple column but for simplicity, we will use only date, summary and overall score. The score helps us to aproximate sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scenario](./img/scenario.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's verify that `SparkSession` is available in the notebook environment. We do not need to explicitly create SparkSession as it is created for us\n",
    "automatically during start of the Jupyter notebook. This works because of the Jupyter is set up with Spark kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.2.43:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109c99a10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare `H2OContext`\n",
    "\n",
    "We will start `H2OContext` in the so called _internal backend_ mode. The means H2O is sharing JVM with Spark (see details in [Sparkling Water documentation](https://github.com/h2oai/sparkling-water/blob/rel-2.2/doc/tutorials/backends.rst)).\n",
    "\n",
    "The following call initializes H2O on each Spark executors in the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://172.16.2.43:54323... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>6 mins 48 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.16.0.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 day </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>sparkling-water-kuba_local-1512176192407</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>6.630 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://172.16.2.43:54323</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.13 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         6 mins 48 secs\n",
       "H2O cluster version:        3.16.0.2\n",
       "H2O cluster version age:    1 day\n",
       "H2O cluster name:           sparkling-water-kuba_local-1512176192407\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    6.630 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://172.16.2.43:54323\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             2.7.13 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * H2O name: sparkling-water-kuba_local-1512176192407\n",
      " * cluster size: 1\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (driver,172.16.2.43,54323)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://172.16.2.43:54323 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the reported IP is private IP of docker container where the demo is running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use H2O to load data using H2O since it does pretty good job to guess all nuances of input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  Id</th><th>ProductId  </th><th>UserId        </th><th>ProfileName                    </th><th style=\"text-align: right;\">  HelpfulnessNumerator</th><th style=\"text-align: right;\">  HelpfulnessDenominator</th><th style=\"text-align: right;\">  Score</th><th style=\"text-align: right;\">       Time</th><th>Summary                                      </th><th>Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">   1</td><td>B001E4KFG0 </td><td>A3SGXH7AUHU8GW</td><td>delmartian                     </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">1.30386e+09</td><td>Good Quality Dog Food                        </td><td>I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.                                                                                                                                                                                                                                                      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   2</td><td>B00813GRG4 </td><td>A1D87F6ZCVE5NK</td><td>dll pa                         </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">1.34698e+09</td><td>Not as Advertised                            </td><td>Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"\"Jumbo\"                                                                                                                                                                                                                                                                                                                               </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   3</td><td>B000LQOCH0 </td><td>ABXLMWJIXXAIN </td><td>Natalia Corres \"\"Natalia Corres</td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">1.21902e+09</td><td>\"\"Delight\"\" says it a                        </td><td>This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"\"The Lion, The Witch, and The Wardrobe\"\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witc</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   4</td><td>B000UA0QIQ </td><td>A395BORC6FGVXV</td><td>Karl                           </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                       3</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">1.30792e+09</td><td>Cough Medicine                               </td><td>If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.                                                                                                                                                                                                                                                                                                  </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   5</td><td>B006K2ZZ7K </td><td>A1UQRSCLF8GW1T</td><td>Michael D. Bigham \"\"M. Wassir  </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">1.35078e+09</td><td>Great taffy                                  </td><td>Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.                                                                                                                                                                                                                                                                                                                                                                                 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   6</td><td>B006K2ZZ7K </td><td>ADT0SRK1MGOEU </td><td>Twoapennything                 </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">1.34205e+09</td><td>Nice Taffy                                   </td><td>I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.                                                                                             </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   7</td><td>B006K2ZZ7K </td><td>A1SP2KVKFXXRU1</td><td>David C. Sullivan              </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">1.34015e+09</td><td>Great!  Just as good as the expensive brands!</td><td>This saltwater taffy had great flavors and was very soft and chewy.  Each candy was individually wrapped well.  None of the candies were stuck together, which did happen in the expensive version, Fralinger's.  Would highly recommend this candy!  I served it at a beach-themed party and everyone loved it!                                                                                                                                                                                                             </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   8</td><td>B006K2ZZ7K </td><td>A3JRGQVEQN31IQ</td><td>Pamela G. Williams             </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">1.336e+09  </td><td>Wonderful, tasty taffy                       </td><td>This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!                                                                                                                                                                                                                                                                                                                                                                                 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   9</td><td>B000E7L2R4 </td><td>A1MZYO9TZK0BBI</td><td>R. James                       </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">1.32201e+09</td><td>Yay Barley                                   </td><td>Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too                                                                                                                                                                                                                                                                                                                                                                                          </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  10</td><td>B00171APVA </td><td>A21BT40VZCCYT4</td><td>Carol A. Reed                  </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">1.35121e+09</td><td>Healthy Dog Food                             </td><td>This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.                                                                                                                                                                                                                                                                                                                                                                                    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h2o\n",
    "reviews_h2o = h2o.upload_file(\"../data/kuba/AmazonReviews.csv\", \"reviews.hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data table in H2O flow\n",
    "\n",
    "At this point, we can explore data directly in this notebook, or we can access H2O Flow and explore data and its properties directly there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Spark frame for the input to the pipeline\n",
    "\n",
    "After data exploration, we can start with data munging. We are going to use Spark, hence we need to publish H2O frame as Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_spark = hc.as_spark_frame(reviews_h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick #1: Save the original Spark schema\n",
    "\n",
    "At this point we will save the schema of input data and we will reuse it later to configure deployed Spark streaming application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-77fac6842efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'schema.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'schema'"
     ]
    }
   ],
   "source": [
    "reviews_spark.printSchema()\n",
    "\n",
    "with open('schema.json','w') as f:\n",
    "    f.write(str(reviews_spark.schema.json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's define all the stages for the pipeline\n",
    "\n",
    "The Spark pipelines are composed of various transformers. In our example we combine a few Spark transformers to clean up textual data and transform it into numerical format. The pipeline is finalized by training an H2O GBM binomial model.\n",
    "\n",
    "> Note: The pipeline stages are not executed right away, the are executed during each fit and transform call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformer to drop unnecessary columns\n",
    "The Spark `SQLTransformer` allows for using SQL to munge data.\n",
    "\n",
    "As part of this transformer, we convert timestamp to the human readable date string:\n",
    "\n",
    "**TODO you need to explain why are you selecting only the subset of columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "colSelect = SQLTransformer(\n",
    "    statement=\"SELECT Score, from_unixtime(Time) as Time, Summary FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick #2: Explore intermediate results\n",
    "To explore intermidiate results,  we can also invoke defined transformer directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------------------+\n",
      "|Score|               Time|             Summary|\n",
      "+-----+-------------------+--------------------+\n",
      "|    5|2011-04-26 17:00:00|Good Quality Dog ...|\n",
      "|    1|2012-09-06 17:00:00|   Not as Advertised|\n",
      "|    4|2008-08-17 17:00:00|\"\"Delight\"\" says ...|\n",
      "|    2|2011-06-12 17:00:00|      Cough Medicine|\n",
      "|    5|2012-10-20 17:00:00|         Great taffy|\n",
      "|    4|2012-07-11 17:00:00|          Nice Taffy|\n",
      "|    5|2012-06-19 17:00:00|Great!  Just as g...|\n",
      "|    5|2012-05-02 17:00:00|Wonderful, tasty ...|\n",
      "|    5|2011-11-22 16:00:00|          Yay Barley|\n",
      "|    5|2012-10-25 17:00:00|    Healthy Dog Food|\n",
      "|    5|2005-02-07 16:00:00|The Best Hot Sauc...|\n",
      "|    5|2010-08-26 17:00:00|My cats LOVE this...|\n",
      "|    1|2012-06-12 17:00:00|My Cats Are Not F...|\n",
      "|    4|2010-11-04 17:00:00|   fresh and greasy!|\n",
      "|    5|2010-03-11 16:00:00|Strawberry Twizzl...|\n",
      "|    5|2009-12-28 16:00:00|Lots of twizzlers...|\n",
      "|    2|2012-09-19 17:00:00|          poor taste|\n",
      "|    5|2012-08-15 17:00:00|            Love it!|\n",
      "|    5|2011-12-22 16:00:00|  GREAT SWEET CANDY!|\n",
      "|    5|2011-10-07 17:00:00|Home delivered tw...|\n",
      "+-----+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = colSelect.transform(reviews_spark)\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create transformer which creates several time columns based on the Time colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "refineTime = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT  Score,\n",
    "            Summary, \n",
    "            dayofmonth(Time) as Day, \n",
    "            month(Time) as Month, year(Time) as Year, \n",
    "            weekofyear(Time) as WeekNum, \n",
    "            date_format(Time, 'EEE') as WeekDay, \n",
    "            hour(Time) as HourOfDay, \n",
    "            IF(date_format(Time, 'EEE')='Sat' OR date_format(Time, 'EEE')='Sun', 1, 0) as Weekend, \n",
    "            CASE \n",
    "                WHEN month(TIME)=12 OR month(Time)<=2 THEN 'Winter' \n",
    "                WHEN month(TIME)>=3 OR month(Time)<=5 THEN 'Spring' \n",
    "                WHEN month(TIME)>=6 AND month(Time)<=9 THEN 'Summer' \n",
    "                ELSE 'Autumn' END as Seasson \n",
    "    FROM __THIS__\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'refineTime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fae5c2b649b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrefined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrefineTime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrefined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'refineTime' is not defined"
     ]
    }
   ],
   "source": [
    "refined = refineTime.transform(selected)\n",
    "refined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove neutral reviews and classify the Scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, IDF, CountVectorizer\n",
    "\n",
    "filterScore = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT  IF(Score<3,'NEGATIVE', 'POSITIVE') as Sentiment, Summary, Day, Month, Year,\n",
    "            WeekNum, WeekDay, HourOfDay, Weekend, Seasson \n",
    "    FROM __THIS__ WHERE Score !=3 \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filterScore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cba0de8b7177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilterScore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filterScore' is not defined"
     ]
    }
   ],
   "source": [
    "filtered = filterScore.transform(refined)\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"Summary\",\n",
    "                                outputCol=\"tokenized_summary\",\n",
    "                                pattern=\"[, ]\",\n",
    "                                toLowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "|Sentiment|             Summary|Day|Month|Year|WeekNum|WeekDay|HourOfDay|Weekend|Seasson|   tokenized_summary|\n",
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "| POSITIVE|Good Quality Dog ...| 26|    4|2011|     17|    Tue|       17|      0| Spring|[good, quality, d...|\n",
      "| NEGATIVE|   Not as Advertised|  6|    9|2012|     36|    Thu|       17|      0| Spring|[not, as, adverti...|\n",
      "| POSITIVE|\"\"Delight\"\" says ...| 17|    8|2008|     33|    Sun|       17|      1| Spring|[\"\"delight\"\", say...|\n",
      "| NEGATIVE|      Cough Medicine| 12|    6|2011|     23|    Sun|       17|      1| Spring|   [cough, medicine]|\n",
      "| POSITIVE|         Great taffy| 20|   10|2012|     42|    Sat|       17|      1| Spring|      [great, taffy]|\n",
      "| POSITIVE|          Nice Taffy| 11|    7|2012|     28|    Wed|       17|      0| Spring|       [nice, taffy]|\n",
      "| POSITIVE|Great!  Just as g...| 19|    6|2012|     25|    Tue|       17|      0| Spring|[great!, just, as...|\n",
      "| POSITIVE|Wonderful, tasty ...|  2|    5|2012|     18|    Wed|       17|      0| Spring|[wonderful, tasty...|\n",
      "| POSITIVE|          Yay Barley| 22|   11|2011|     47|    Tue|       16|      0| Spring|       [yay, barley]|\n",
      "| POSITIVE|    Healthy Dog Food| 25|   10|2012|     43|    Thu|       17|      0| Spring|[healthy, dog, food]|\n",
      "| POSITIVE|The Best Hot Sauc...|  7|    2|2005|      6|    Mon|       16|      0| Winter|[the, best, hot, ...|\n",
      "| POSITIVE|My cats LOVE this...| 26|    8|2010|     34|    Thu|       17|      0| Spring|[my, cats, love, ...|\n",
      "| NEGATIVE|My Cats Are Not F...| 12|    6|2012|     24|    Tue|       17|      0| Spring|[my, cats, are, n...|\n",
      "| POSITIVE|   fresh and greasy!|  4|   11|2010|     44|    Thu|       17|      0| Spring|[fresh, and, grea...|\n",
      "| POSITIVE|Strawberry Twizzl...| 11|    3|2010|     10|    Thu|       16|      0| Spring|[strawberry, twiz...|\n",
      "| POSITIVE|Lots of twizzlers...| 28|   12|2009|     53|    Mon|       16|      0| Winter|[lots, of, twizzl...|\n",
      "| NEGATIVE|          poor taste| 19|    9|2012|     38|    Wed|       17|      0| Spring|       [poor, taste]|\n",
      "| POSITIVE|            Love it!| 15|    8|2012|     33|    Wed|       17|      0| Spring|         [love, it!]|\n",
      "| POSITIVE|  GREAT SWEET CANDY!| 22|   12|2011|     51|    Thu|       16|      0| Winter|[great, sweet, ca...|\n",
      "| POSITIVE|Home delivered tw...|  7|   10|2011|     40|    Fri|       17|      0| Spring|[home, delivered,...|\n",
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = regexTokenizer.transform(filtered)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWordsRemover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(),\n",
    "                                    outputCol=\"CleanedSummary\",\n",
    "                                    caseSensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|Sentiment|             Summary|      CleanedSummary|\n",
      "+---------+--------------------+--------------------+\n",
      "| POSITIVE|Good Quality Dog ...|[good, quality, d...|\n",
      "| NEGATIVE|   Not as Advertised|        [advertised]|\n",
      "| POSITIVE|\"\"Delight\"\" says ...| [\"\"delight\"\", says]|\n",
      "| NEGATIVE|      Cough Medicine|   [cough, medicine]|\n",
      "| POSITIVE|         Great taffy|      [great, taffy]|\n",
      "| POSITIVE|          Nice Taffy|       [nice, taffy]|\n",
      "| POSITIVE|Great!  Just as g...|[great!, good, ex...|\n",
      "| POSITIVE|Wonderful, tasty ...|[wonderful, tasty...|\n",
      "| POSITIVE|          Yay Barley|       [yay, barley]|\n",
      "| POSITIVE|    Healthy Dog Food|[healthy, dog, food]|\n",
      "| POSITIVE|The Best Hot Sauc...|[best, hot, sauce...|\n",
      "| POSITIVE|My cats LOVE this...|[cats, love, \"\"di...|\n",
      "| NEGATIVE|My Cats Are Not F...|[cats, fans, new,...|\n",
      "| POSITIVE|   fresh and greasy!|    [fresh, greasy!]|\n",
      "| POSITIVE|Strawberry Twizzl...|[strawberry, twiz...|\n",
      "| POSITIVE|Lots of twizzlers...|[lots, twizzlers,...|\n",
      "| NEGATIVE|          poor taste|       [poor, taste]|\n",
      "| POSITIVE|            Love it!|         [love, it!]|\n",
      "| POSITIVE|  GREAT SWEET CANDY!|[great, sweet, ca...|\n",
      "| POSITIVE|Home delivered tw...|[home, delivered,...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopWordsRemoved = stopWordsRemover.transform(tokenized)\n",
    "stopWordsRemoved.select([\"Sentiment\", \"Summary\", \"CleanedSummary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(inputCol=stopWordsRemover.getOutputCol(),\n",
    "                                  outputCol=\"frequencies\",\n",
    "                                  minDF=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually train the count vectorizer just for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countVecModel = countVectorizer.fit(stopWordsRemoved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 1767\n",
      "[u'great', u'good', u'best', u'love', u'coffee', u'tea', u'product', u'taste', u'delicious', u'excellent']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size is \" + str(len(countVecModel.vocabulary)))\n",
    "\n",
    "print(countVecModel.vocabulary[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|Sentiment|      CleanedSummary|         frequencies|\n",
      "+---------+--------------------+--------------------+\n",
      "| POSITIVE|[good, quality, d...|(1767,[1,10,12,35...|\n",
      "| NEGATIVE|        [advertised]|  (1767,[605],[1.0])|\n",
      "| POSITIVE| [\"\"delight\"\", says]|  (1767,[422],[1.0])|\n",
      "| NEGATIVE|   [cough, medicine]| (1767,[1700],[1.0])|\n",
      "| POSITIVE|      [great, taffy]|(1767,[0,1461],[1...|\n",
      "| POSITIVE|       [nice, taffy]|(1767,[29,1461],[...|\n",
      "| POSITIVE|[great!, good, ex...|(1767,[1,62,130],...|\n",
      "| POSITIVE|[wonderful, tasty...|(1767,[15,37,1461...|\n",
      "| POSITIVE|       [yay, barley]|        (1767,[],[])|\n",
      "| POSITIVE|[healthy, dog, food]|(1767,[10,12,21],...|\n",
      "| POSITIVE|[best, hot, sauce...|(1767,[2,43,87,42...|\n",
      "| POSITIVE|[cats, love, \"\"di...|(1767,[3,12,23,44...|\n",
      "| NEGATIVE|[cats, fans, new,...|(1767,[12,44,80,1...|\n",
      "| POSITIVE|    [fresh, greasy!]|   (1767,[85],[1.0])|\n",
      "| POSITIVE|[strawberry, twiz...|(1767,[13,19,692]...|\n",
      "| POSITIVE|[lots, twizzlers,...|  (1767,[404],[1.0])|\n",
      "| NEGATIVE|       [poor, taste]|(1767,[7,185],[1....|\n",
      "| POSITIVE|         [love, it!]|(1767,[3,32],[1.0...|\n",
      "| POSITIVE|[great, sweet, ca...|(1767,[0,41,911],...|\n",
      "| POSITIVE|[home, delivered,...|(1767,[354,1095],...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized = countVecModel.transform(stopWordsRemoved)\n",
    "vectorized.select([\"Sentiment\", \"CleanedSummary\", \"frequencies\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create inverse document frequencies model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=countVectorizer.getOutputCol(),\n",
    "          outputCol=\"tf_idf_frequencies\",\n",
    "          minDocFreq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually train the IDF model just for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel = idf.fit(vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+\n",
      "|Sentiment|      CleanedSummary|         frequencies|  tf_idf_frequencies|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "| POSITIVE|[good, quality, d...|(1767,[1,10,12,35...|(1767,[1,10,12,35...|\n",
      "| NEGATIVE|        [advertised]|  (1767,[605],[1.0])|(1767,[605],[7.30...|\n",
      "| POSITIVE| [\"\"delight\"\", says]|  (1767,[422],[1.0])|(1767,[422],[6.91...|\n",
      "| NEGATIVE|   [cough, medicine]| (1767,[1700],[1.0])|(1767,[1700],[8.5...|\n",
      "| POSITIVE|      [great, taffy]|(1767,[0,1461],[1...|(1767,[0,1461],[2...|\n",
      "| POSITIVE|       [nice, taffy]|(1767,[29,1461],[...|(1767,[29,1461],[...|\n",
      "| POSITIVE|[great!, good, ex...|(1767,[1,62,130],...|(1767,[1,62,130],...|\n",
      "| POSITIVE|[wonderful, tasty...|(1767,[15,37,1461...|(1767,[15,37,1461...|\n",
      "| POSITIVE|       [yay, barley]|        (1767,[],[])|        (1767,[],[])|\n",
      "| POSITIVE|[healthy, dog, food]|(1767,[10,12,21],...|(1767,[10,12,21],...|\n",
      "| POSITIVE|[best, hot, sauce...|(1767,[2,43,87,42...|(1767,[2,43,87,42...|\n",
      "| POSITIVE|[cats, love, \"\"di...|(1767,[3,12,23,44...|(1767,[3,12,23,44...|\n",
      "| NEGATIVE|[cats, fans, new,...|(1767,[12,44,80,1...|(1767,[12,44,80,1...|\n",
      "| POSITIVE|    [fresh, greasy!]|   (1767,[85],[1.0])|(1767,[85],[5.528...|\n",
      "| POSITIVE|[strawberry, twiz...|(1767,[13,19,692]...|(1767,[13,19,692]...|\n",
      "| POSITIVE|[lots, twizzlers,...|  (1767,[404],[1.0])|(1767,[404],[6.87...|\n",
      "| NEGATIVE|       [poor, taste]|(1767,[7,185],[1....|(1767,[7,185],[3....|\n",
      "| POSITIVE|         [love, it!]|(1767,[3,32],[1.0...|(1767,[3,32],[3.0...|\n",
      "| POSITIVE|[great, sweet, ca...|(1767,[0,41,911],...|(1767,[0,41,911],...|\n",
      "| POSITIVE|[home, delivered,...|(1767,[354,1095],...|(1767,[354,1095],...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "afterIdf = idfModel.transform(vectorized)\n",
    "afterIdf.select([\"Sentiment\", \"CleanedSummary\", \"frequencies\", \"tf_idf_frequencies\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Remove Summary Column\n",
    "\n",
    "The algoritm does not understand the string value. That's why we transformed the data using TF-IDF and should drop the original string information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "removeSummary = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT Sentiment, Day, Month, Year, WeekNum, WeekDay, HourOfDay, Weekend, Seasson, tf_idf_frequencies\n",
    "    FROM __THIS__ \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|WeekDay|HourOfDay|Weekend|Seasson|  tf_idf_frequencies|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "| POSITIVE| 26|    4|2011|     17|    Tue|       17|      0| Spring|(1767,[1,10,12,35...|\n",
      "| NEGATIVE|  6|    9|2012|     36|    Thu|       17|      0| Spring|(1767,[605],[7.30...|\n",
      "| POSITIVE| 17|    8|2008|     33|    Sun|       17|      1| Spring|(1767,[422],[6.91...|\n",
      "| NEGATIVE| 12|    6|2011|     23|    Sun|       17|      1| Spring|(1767,[1700],[8.5...|\n",
      "| POSITIVE| 20|   10|2012|     42|    Sat|       17|      1| Spring|(1767,[0,1461],[2...|\n",
      "| POSITIVE| 11|    7|2012|     28|    Wed|       17|      0| Spring|(1767,[29,1461],[...|\n",
      "| POSITIVE| 19|    6|2012|     25|    Tue|       17|      0| Spring|(1767,[1,62,130],...|\n",
      "| POSITIVE|  2|    5|2012|     18|    Wed|       17|      0| Spring|(1767,[15,37,1461...|\n",
      "| POSITIVE| 22|   11|2011|     47|    Tue|       16|      0| Spring|        (1767,[],[])|\n",
      "| POSITIVE| 25|   10|2012|     43|    Thu|       17|      0| Spring|(1767,[10,12,21],...|\n",
      "| POSITIVE|  7|    2|2005|      6|    Mon|       16|      0| Winter|(1767,[2,43,87,42...|\n",
      "| POSITIVE| 26|    8|2010|     34|    Thu|       17|      0| Spring|(1767,[3,12,23,44...|\n",
      "| NEGATIVE| 12|    6|2012|     24|    Tue|       17|      0| Spring|(1767,[12,44,80,1...|\n",
      "| POSITIVE|  4|   11|2010|     44|    Thu|       17|      0| Spring|(1767,[85],[5.528...|\n",
      "| POSITIVE| 11|    3|2010|     10|    Thu|       16|      0| Spring|(1767,[13,19,692]...|\n",
      "| POSITIVE| 28|   12|2009|     53|    Mon|       16|      0| Winter|(1767,[404],[6.87...|\n",
      "| NEGATIVE| 19|    9|2012|     38|    Wed|       17|      0| Spring|(1767,[7,185],[3....|\n",
      "| POSITIVE| 15|    8|2012|     33|    Wed|       17|      0| Spring|(1767,[3,32],[3.0...|\n",
      "| POSITIVE| 22|   12|2011|     51|    Thu|       16|      0| Winter|(1767,[0,41,911],...|\n",
      "| POSITIVE|  7|   10|2011|     40|    Fri|       17|      0| Spring|(1767,[354,1095],...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removedSummary = removeSummary.transform(afterIdf)\n",
    "removedSummary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pysparkling.ml import ColumnPruner, H2OGBM\n",
    "\n",
    "gbm = H2OGBM(ratio=0.8,\n",
    "             featuresCols=[idf.getOutputCol()],\n",
    "             predictionCol=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the pipeline by defining all the stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[colSelect,\n",
    "                            refineTime,\n",
    "                            filterScore,\n",
    "                            regexTokenizer,\n",
    "                            stopWordsRemover,\n",
    "                            countVectorizer,\n",
    "                            idf,\n",
    "                            removeSummary,\n",
    "                            gbm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(reviews_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try predicting on the same input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+--------------------+-------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|WeekDay|HourOfDay|Weekend|Seasson|  tf_idf_frequencies|            NEGATIVE|           POSITIVE|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+--------------------+-------------------+\n",
      "| POSITIVE| 26|    4|2011|     17|    Tue|       17|      0| Spring|(1767,[1,10,12,35...| 0.11963043647646754| 0.8803695635235325|\n",
      "| NEGATIVE|  6|    9|2012|     36|    Thu|       17|      0| Spring|(1767,[605],[7.30...|  0.1822741616671273| 0.8177258383328727|\n",
      "| POSITIVE| 17|    8|2008|     33|    Sun|       17|      1| Spring|(1767,[422],[6.91...|  0.1822741616671273| 0.8177258383328727|\n",
      "| NEGATIVE| 12|    6|2011|     23|    Sun|       17|      1| Spring|(1767,[1700],[8.5...|  0.1822741616671273| 0.8177258383328727|\n",
      "| POSITIVE| 20|   10|2012|     42|    Sat|       17|      1| Spring|(1767,[0,1461],[2...| 0.05166565827398828| 0.9483343417260117|\n",
      "| POSITIVE| 11|    7|2012|     28|    Wed|       17|      0| Spring|(1767,[29,1461],[...|  0.1668990714089611| 0.8331009285910389|\n",
      "| POSITIVE| 19|    6|2012|     25|    Tue|       17|      0| Spring|(1767,[1,62,130],...| 0.11963043647646754| 0.8803695635235325|\n",
      "| POSITIVE|  2|    5|2012|     18|    Wed|       17|      0| Spring|(1767,[15,37,1461...| 0.12363429328215514| 0.8763657067178449|\n",
      "| POSITIVE| 22|   11|2011|     47|    Tue|       16|      0| Spring|        (1767,[],[])|  0.1822741616671273| 0.8177258383328727|\n",
      "| POSITIVE| 25|   10|2012|     43|    Thu|       17|      0| Spring|(1767,[10,12,21],...|  0.1822741616671273| 0.8177258383328727|\n",
      "| POSITIVE|  7|    2|2005|      6|    Mon|       16|      0| Winter|(1767,[2,43,87,42...|0.058676541625967804| 0.9413234583740322|\n",
      "| POSITIVE| 26|    8|2010|     34|    Thu|       17|      0| Spring|(1767,[3,12,23,44...| 0.07538343793312874| 0.9246165620668713|\n",
      "| NEGATIVE| 12|    6|2012|     24|    Tue|       17|      0| Spring|(1767,[12,44,80,1...|  0.1822741616671273| 0.8177258383328727|\n",
      "| POSITIVE|  4|   11|2010|     44|    Thu|       17|      0| Spring|(1767,[85],[5.528...|  0.1822741616671273| 0.8177258383328727|\n",
      "| POSITIVE| 11|    3|2010|     10|    Thu|       16|      0| Spring|(1767,[13,19,692]...|  0.1223020102606922| 0.8776979897393078|\n",
      "| POSITIVE| 28|   12|2009|     53|    Mon|       16|      0| Winter|(1767,[404],[6.87...|  0.1822741616671273| 0.8177258383328727|\n",
      "| NEGATIVE| 19|    9|2012|     38|    Wed|       17|      0| Spring|(1767,[7,185],[3....|  0.6822770401907026|0.31772295980929743|\n",
      "| POSITIVE| 15|    8|2012|     33|    Wed|       17|      0| Spring|(1767,[3,32],[3.0...| 0.07538343793312874| 0.9246165620668713|\n",
      "| POSITIVE| 22|   12|2011|     51|    Thu|       16|      0| Winter|(1767,[0,41,911],...| 0.05166565827398828| 0.9483343417260117|\n",
      "| POSITIVE|  7|   10|2011|     40|    Fri|       17|      0| Spring|(1767,[354,1095],...|  0.1822741616671273| 0.8177258383328727|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(reviews_spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save the pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"reviews_pipeline.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'great'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[5].vocabulary[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySparkling",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
