{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.2.43:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10e6a6a10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that SparkSession is available. We do not need to explicitly create SparkSession as it is created for us\n",
    "# automatically during start of the Jupiter notebook. This works because of the Jupiter is set up with Spark kernel.\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://172.16.2.43:54323... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>11 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.16.0.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>23 hours and 40 minutes </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>sparkling-water-kuba_local-1512169151902</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>6.975 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://172.16.2.43:54323</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.13 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         11 secs\n",
       "H2O cluster version:        3.16.0.2\n",
       "H2O cluster version age:    23 hours and 40 minutes\n",
       "H2O cluster name:           sparkling-water-kuba_local-1512169151902\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    6.975 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://172.16.2.43:54323\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             2.7.13 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * H2O name: sparkling-water-kuba_local-1512169151902\n",
      " * cluster size: 1\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (driver,172.16.2.43,54323)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://172.16.2.43:54323 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Start H2OContext in the internal backend.\n",
    "\n",
    "# This call initializes H2O on each Spark executors in the Spark cluster.\n",
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Load data using H2O because we belive in our CSV parser correctly identifies the schema\n",
    "import h2o\n",
    "reviews_h2o = h2o.upload_file(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to Spark frame for the input to the pipeline\n",
    "reviews_spark = hc.as_spark_frame(reviews_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = false)\n",
      " |-- ProductId: string (nullable = false)\n",
      " |-- UserId: string (nullable = false)\n",
      " |-- ProfileName: string (nullable = false)\n",
      " |-- HelpfulnessNumerator: short (nullable = false)\n",
      " |-- HelpfulnessDenominator: short (nullable = false)\n",
      " |-- Score: byte (nullable = false)\n",
      " |-- Time: integer (nullable = false)\n",
      " |-- Summary: string (nullable = false)\n",
      " |-- Text: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the original spark schema. We will reuse this schema during online streaming for each row on the input. \n",
    "# This can be handy as during data streaming, it can be hard to correctly identify the type of the data on the input \n",
    "# because it is just a few rows. This allows us to use the schema from the training time during the production time to\n",
    "# maximize the best results\n",
    "\n",
    "reviews_spark.printSchema()\n",
    "\n",
    "f = open('schema.json','w')\n",
    "f.write(str(reviews_spark.schema.json()))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Score=5, Time=u'2011-04-26 17:00:00', Summary=u'Good Quality Dog Food'),\n",
       " Row(Score=1, Time=u'2012-09-06 17:00:00', Summary=u'Not as Advertised'),\n",
       " Row(Score=4, Time=u'2008-08-17 17:00:00', Summary=u'\"\"Delight\"\" says it a'),\n",
       " Row(Score=2, Time=u'2011-06-12 17:00:00', Summary=u'Cough Medicine'),\n",
       " Row(Score=5, Time=u'2012-10-20 17:00:00', Summary=u'Great taffy'),\n",
       " Row(Score=4, Time=u'2012-07-11 17:00:00', Summary=u'Nice Taffy'),\n",
       " Row(Score=5, Time=u'2012-06-19 17:00:00', Summary=u'Great!  Just as good as the expensive brands!'),\n",
       " Row(Score=5, Time=u'2012-05-02 17:00:00', Summary=u'Wonderful, tasty taffy'),\n",
       " Row(Score=5, Time=u'2011-11-22 16:00:00', Summary=u'Yay Barley'),\n",
       " Row(Score=5, Time=u'2012-10-25 17:00:00', Summary=u'Healthy Dog Food')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we define all the stages for the pipeline\n",
    "# The pipeline stages are not executed right away, the are executed during each fit and transform call.\n",
    "\n",
    "# Define transformer to drop unnecessary columns\n",
    "# Here, we use SQLTransformer which allows us to transform data using Spark SQL.\n",
    "\n",
    "# As part of this transformer, we convert timestamp to the human readable date string\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "colSelect = SQLTransformer(\n",
    "    statement=\"SELECT Score, from_unixtime(Time) as Time, Summary FROM __THIS__\")\n",
    "\n",
    "# The pipeline transformers are automatically invoked during the pipeline execution, however we can also call them\n",
    "# directly just to see the intermediate results\n",
    "selected = colSelect.transform(reviews_spark)\n",
    "selected.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Score=5, Summary=u'Good Quality Dog Food', Day=26, Month=4, Year=2011, WeekNum=17, WeekDay=u'Tue', HourOfDay=17, Weekend=0, Seasson=u'Spring')\n",
      "\n",
      "root\n",
      " |-- Score: byte (nullable = false)\n",
      " |-- Summary: string (nullable = false)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- WeekNum: integer (nullable = true)\n",
      " |-- WeekDay: string (nullable = true)\n",
      " |-- HourOfDay: integer (nullable = true)\n",
      " |-- Weekend: integer (nullable = false)\n",
      " |-- Seasson: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create transformer which creates several time columns based on the Time column\n",
    "refineTime = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT  Score,\n",
    "            Summary, \n",
    "            dayofmonth(Time) as Day, \n",
    "            month(Time) as Month, year(Time) as Year, \n",
    "            weekofyear(Time) as WeekNum, \n",
    "            date_format(Time, 'EEE') as WeekDay, \n",
    "            hour(Time) as HourOfDay, \n",
    "            IF(date_format(Time, 'EEE')='Sat' OR date_format(Time, 'EEE')='Sun', 1, 0) as Weekend, \n",
    "            CASE \n",
    "                WHEN month(TIME)=12 OR month(Time)<=2 THEN 'Winter' \n",
    "                WHEN month(TIME)>=3 OR month(Time)<=5 THEN 'Spring' \n",
    "                WHEN month(TIME)>=6 AND month(Time)<=9 THEN 'Summer' \n",
    "                ELSE 'Autumn' END as Seasson \n",
    "    FROM __THIS__\"\"\")\n",
    "\n",
    "# Just inspect the data after \n",
    "refined = refineTime.transform(selected)\n",
    "print(refined.head())\n",
    "print(\"\")\n",
    "# Show the schema\n",
    "refined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Sentiment=u'POSITIVE', Summary=u'Good Quality Dog Food', Day=26, Month=4, Year=2011, WeekNum=17, WeekDay=u'Tue', HourOfDay=17, Weekend=0, Seasson=u'Spring')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove neutral reviews and classify the Scores\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, IDF, CountVectorizer\n",
    "\n",
    "filterScore = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT  IF(Score<3,'NEGATIVE', 'POSITIVE') as Sentiment, Summary, Day, Month, Year,\n",
    "            WeekNum, WeekDay, HourOfDay, Weekend, Seasson \n",
    "    FROM __THIS__ WHERE Score !=3 \"\"\")\n",
    "\n",
    "# Inspect the data\n",
    "filtered = filterScore.transform(refined)\n",
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Sentiment=u'POSITIVE', Summary=u'Good Quality Dog Food', Day=26, Month=4, Year=2011, WeekNum=17, WeekDay=u'Tue', HourOfDay=17, Weekend=0, Seasson=u'Spring', TokenizedSummary=[u'good', u'quality', u'dog', u'food'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the message\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Summary\",\n",
    "                                outputCol=\"TokenizedSummary\",\n",
    "                                pattern=\"[, ]\",\n",
    "                                toLowercase=True)\n",
    "\n",
    "# Inspect the data\n",
    "tokenized = regexTokenizer.transform(filtered)\n",
    "tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Sentiment=u'POSITIVE', Summary=u'Good Quality Dog Food', Day=26, Month=4, Year=2011, WeekNum=17, WeekDay=u'Tue', HourOfDay=17, Weekend=0, Seasson=u'Spring', TokenizedSummary=[u'good', u'quality', u'dog', u'food'], CleanedSummary=[u'good', u'quality', u'dog', u'food'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unnecessary words\n",
    "stopWordsRemover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(),\n",
    "                                    outputCol=\"CleanedSummary\",\n",
    "                                    caseSensitive=False)\n",
    "\n",
    "# Inspect the data\n",
    "stopWordsRemoved = stopWordsRemover.transform(tokenized)\n",
    "stopWordsRemoved.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 68859\n",
      "[u'great', u'good', u'best', u'love', u'coffee', u'tea', u'product', u'taste', u'delicious', u'excellent']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Sentiment=u'POSITIVE', Summary=u'Good Quality Dog Food', Day=26, Month=4, Year=2011, WeekNum=17, WeekDay=u'Tue', HourOfDay=17, Weekend=0, Seasson=u'Spring', TokenizedSummary=[u'good', u'quality', u'dog', u'food'], CleanedSummary=[u'good', u'quality', u'dog', u'food'], wordToIndex=SparseVector(68859, {1: 1.0, 10: 1.0, 12: 1.0, 35: 1.0})),\n",
       " Row(Sentiment=u'NEGATIVE', Summary=u'Not as Advertised', Day=6, Month=9, Year=2012, WeekNum=36, WeekDay=u'Thu', HourOfDay=17, Weekend=0, Seasson=u'Spring', TokenizedSummary=[u'not', u'as', u'advertised'], CleanedSummary=[u'advertised'], wordToIndex=SparseVector(68859, {605: 1.0})),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'\"\"Delight\"\" says it a', Day=17, Month=8, Year=2008, WeekNum=33, WeekDay=u'Sun', HourOfDay=17, Weekend=1, Seasson=u'Spring', TokenizedSummary=[u'\"\"delight\"\"', u'says', u'it', u'a'], CleanedSummary=[u'\"\"delight\"\"', u'says'], wordToIndex=SparseVector(68859, {422: 1.0, 42239: 1.0})),\n",
       " Row(Sentiment=u'NEGATIVE', Summary=u'Cough Medicine', Day=12, Month=6, Year=2011, WeekNum=23, WeekDay=u'Sun', HourOfDay=17, Weekend=1, Seasson=u'Spring', TokenizedSummary=[u'cough', u'medicine'], CleanedSummary=[u'cough', u'medicine'], wordToIndex=SparseVector(68859, {1704: 1.0, 1809: 1.0})),\n",
       " Row(Sentiment=u'POSITIVE', Summary=u'Great taffy', Day=20, Month=10, Year=2012, WeekNum=42, WeekDay=u'Sat', HourOfDay=17, Weekend=1, Seasson=u'Spring', TokenizedSummary=[u'great', u'taffy'], CleanedSummary=[u'great', u'taffy'], wordToIndex=SparseVector(68859, {0: 1.0, 1461: 1.0}))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hash the words\n",
    "countVectorizer = CountVectorizer(inputCol=stopWordsRemover.getOutputCol(),\n",
    "                      outputCol=\"wordToIndex\")\n",
    "\n",
    "# Manually train the count vectorizer\n",
    "countVecModel = countVectorizer.fit(stopWordsRemoved)\n",
    "# See the vocabulary\n",
    "print(\"Vocabulary size is \" + str(len(countVecModel.vocabulary)))\n",
    "\n",
    "print(countVecModel.vocabulary[:10])\n",
    "# Inspect the data\n",
    "vectorized = countVecModel.transform(stopWordsRemoved)\n",
    "vectorized.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create inverse document frequencies model\n",
    "idf = IDF(inputCol=countVectorizer.getOutputCol(),\n",
    "          outputCol=\"tf_idf\",\n",
    "          minDocFreq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pysparkling.ml import ColumnPruner, H2OGBM\n",
    "\n",
    "# Create GBM model\n",
    "gbm = H2OGBM(ratio=0.8,\n",
    "             featuresCols=[idf.getOutputCol()],\n",
    "             predictionCol=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all helper columns\n",
    "colPruner = ColumnPruner(columns=[\n",
    "        idf.getOutputCol(),\n",
    "        stopWordsRemover.getOutputCol(),\n",
    "        regexTokenizer.getOutputCol()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Create the pipeline by defining all the stages\n",
    "pipeline = Pipeline(stages=[colSelect,\n",
    "                            refineTime,\n",
    "                            filterScore,\n",
    "                            regexTokenizer,\n",
    "                            stopWordsRemover,\n",
    "                            countVectorizer,\n",
    "                            idf,\n",
    "                            gbm,\n",
    "                            colPruner])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the pipeline model\n",
    "model = pipeline.fit(reviews_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try predicting on the same input data\n",
    "model.transform(reviews_spark).take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the pipeline model\n",
    "model.write().overwrite().save(\"Pipeline.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
