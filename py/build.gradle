import static groovy.io.FileType.FILES
import ru.vyarus.gradle.plugin.python.cmd.Python

apply from: "$rootDir/gradle/utils.gradle"

apply plugin: 'ru.vyarus.use-python'
ext {
    FS = File.separator
    FPS = File.pathSeparator
    pythonexec = findProperty("pythonExec") ?: "python"
    pkgDir = file("$buildDir/pkg")
    distDir = file("$buildDir/dist")
}

def getPythonVersion() {
    Python p = new Python(project, python.getPythonPath(), python.getPythonBinary())
    return p.version
}

def getPythonMajorVersion() {
    Python p = new Python(project, python.getPythonPath(), python.getPythonBinary())
    String[] split = p.version.split("\\.")
    return split[0] + "." + split[1]
}

python {
    pip "tabulate:0.8.3"
    pip "requests:2.21.0"
    pip "future:0.17.1"
    pip "colorama:0.3.8"
    pip "six:1.12.0"
    pip "numpy:1.16.2"
    pip "pyspark:${sparkVersion}"
    envPath "${rootDir.toString()}/.gradle/python/${getPythonVersion()}/${sparkVersion}"
}


description = "PySparkling - Sparkling-Water Python Package"

configurations {
    mojopipeline
    s3deps
}

dependencies {
    compile project(path: ':sparkling-water-assembly')

    if (project.property("testMojoPipeline") == "true") {
        mojopipeline "ai.h2o:mojo2-runtime-impl:${mojoPipelineVersion}"
    }

    s3deps "com.amazonaws:aws-java-sdk:1.7.4"
    s3deps "org.apache.hadoop:hadoop-aws:2.7.3"
}

task resolveTestRuntimeDependencies {
    doLast {
        project.configurations.mojopipeline.resolve()
    }
}

//
// Create a file with version for Python dist task
//
task createVersionFile {
    doLast {
        def versionFileDir = new File(pkgDir, "pysparkling")
        if (!versionFileDir.exists()) {
            versionFileDir.mkdirs()
        }
        File version_file = new File(versionFileDir, "version.txt")
        def version_txt = version.replace("-SNAPSHOT","")
        version_file.write(version_txt)
    }
}

//
// Represents a location of H2O Wheel Package
//
def h2oPythonWheelPackageLocation = "http://h2o-release.s3.amazonaws.com/h2o/${h2oMajorName != "master" ? "rel-${h2oMajorName}" : "master"}/${h2oBuild}/Python/h2o-${h2oMajorVersion}.${h2oBuild}-py2.py3-none-any.whl"


//
// Prepare Python environment to run python tests
//
def preparePythonTestEnv(environment) {

    // Add PySparkling zip on PYTHONPATH so unittests can access PySparkling package
    environment['PYTHONPATH'] = "${distPython.archiveFile.get()}:${python.getEnvPath()}/lib/python${getPythonMajorVersion()}/site-packages"
    environment['spark.ext.h2o.backend.cluster.mode'] = detectBackendClusterMode()
    environment["spark.ext.h2o.external.start.mode"] = detectExternalBackendStartMode()
    // sparkling assembly jar is here because of external h2o tests
    environment["sparkling.assembly.jar"] = configurations.compile.join(',')

    environment['SPARK_WORKER_DIR'] = file('build/h2ologs-pyunit/nodes')
    environment['SPARK_CONF_DIR'] = file("tests/conf/pyunit")

    if (project.property("testMojoPipeline") == "true") {
        environment["sparkling.mojo.pipeline.jar"] = configurations.mojopipeline.resolve().join(",")
    }
    gradle.taskGraph.whenReady {
        if (gradle.taskGraph.hasTask(":${project.name}:hadoopSmokeTests")) {
            environment["sparkling.s3.jars"] = configurations.s3deps.resolve().join(",")
        }
    }
}
//
// Initial task checking setup of all properties required
// by Python build
//
task checkPythonEnv {
    doLast {
        def H2O_HOME = System.getenv("H2O_HOME")
        def H2O_PYTHON_WHEEL = System.getenv("H2O_PYTHON_WHEEL")
        def H2O_EXTENDED_JAR = System.getenv("H2O_EXTENDED_JAR")


        if (H2O_HOME == null && H2O_PYTHON_WHEEL == null) {
            throw new InvalidUserDataException("""
    Both properties H2O_HOME and H2O_PYTHON_WHEEL were not found!

    Please specify:
     - H2O_HOME to point to H2O Git repo version ${h2oMajorVersion}.${h2oBuild}
    or
     - H2O_PYTHON_WHEEL to point to downloaded H2O Python Wheel package version ${h2oMajorVersion}.${h2oBuild}
       For example:

        mkdir -p \$(pwd)/private/
        curl -s ${h2oPythonWheelPackageLocation} > \$(pwd)/private/h2o.whl
        export H2O_PYTHON_WHEEL=\$(pwd)/private/h2o.whl
    """)
        }

        // if the spark.ext.h2o.backend.cluster.mode is set to external, then
        // we need to have also H2O_EXTENDED_JAR property set in order to be able to perform tests
        if (detectBackendClusterMode() == "external" && H2O_EXTENDED_JAR == null) {
            throw new InvalidUserDataException("""When running tests on external H2O cluster, H2O_EXTENDED_JAR property is required.

Please set it, for example as:

export H2O_EXTENDED_JAR=`./gradlew -q extendJar -PdownloadH2O`
                                  """)
        }


        if (H2O_HOME != null && H2O_PYTHON_WHEEL != null) {
            logger.info("Both \"H2O_HOME\" and \"H2O_PYTHON_WHEEL\" properties are set. Using \"H2O_HOME\"!")
        }
    }
}

def copyPySetup() {
    copy {
        from("$projectDir") {
            include 'README.rst'
            include 'MANIFEST.in'
            include 'setup.cfg'
            include 'setup.py'
            include 'pysparkling/**/*'
            include 'py_sparkling/**/*'
            exclude '**/*.pyc'
        }
        into pkgDir
    }
    copy {
        from("$projectDir") {
            include 'conda/**/*'
        }
        into buildDir
    }
}


def substituteVersionsInConda() {
        def condaDir = new File("${buildDir}/conda/h2o_pysparkling_SUBST_SPARK_MAJOR_VERSION")
        def renamedCondaDir = new File("${buildDir}/conda/h2o_pysparkling_${sparkMajorVersion}")
        condaDir.renameTo(renamedCondaDir)

        def yamlConf = new File("${renamedCondaDir.toString()}/meta.yaml")
        def contents = yamlConf.getText('UTF-8')
        contents = contents
                .replaceAll("SUBST_SPARK_MAJOR_VERSION", sparkMajorVersion)
                .replaceAll("SUBST_SPARK_VERSION", sparkVersion)
                .replaceAll("SUBST_PYSPARKLING_VERSION", version.replace("-SNAPSHOT",""))
        yamlConf.write(contents, 'UTF-8')
}


def substituteInSetup() {
    def setup = new File("${pkgDir}/setup.py")

    def (first, second, third) = version.replace("-SNAPSHOT","").tokenize(".")
    def majorVersion = "${first}.${second}"

    def contents = setup.getText('UTF-8')
    contents = contents
            .replaceAll("SUBST_SPARK_MAJOR_VERSION", sparkMajorVersion)
            .replaceAll("SUBST_SPARK_VERSION", sparkVersion)
            .replaceAll("SUBST_PYSPARKLING_VERSION", version.replace("-SNAPSHOT",""))
            .replaceAll("SUBST_PYSPARKLING_MAJOR_VERSION", majorVersion)
    setup.write(contents, 'UTF-8')
}



def copyH2OFromH2OHome(String h2oHome) {
        copy {
            from "${h2oHome}/h2o-py/h2o"
            into file("${project.pkgDir}/h2o")
            exclude '**/*.pyc'
            exclude '**/h2o.jar'
        }
}

def copyH2OFromH2OWheel(String h2oPythonWheel) {
        copy {
            from zipTree(h2oPythonWheel)
            into file("${project.pkgDir}")
            include 'h2o/**'
            exclude '**/*.pyc'
            exclude '**/h2o.jar'
        }
}

//
// Make PySparkling distribution zip package
//
task distPython(type: Zip, dependsOn: [clean, checkPythonEnv, configurations.compile] ) {
    doFirst {
        def H2O_HOME = System.getenv("H2O_HOME")
        def H2O_PYTHON_WHEEL = System.getenv("H2O_PYTHON_WHEEL")

        // if both properties are set, give precedence to H2O_HOME
        if (H2O_HOME !=null && H2O_PYTHON_WHEEL !=null) {
            copyH2OFromH2OHome(H2O_HOME)
        } else if (H2O_HOME != null) {
            copyH2OFromH2OHome(H2O_HOME)
        } else if (H2O_PYTHON_WHEEL != null) {
            copyH2OFromH2OWheel(H2O_PYTHON_WHEEL)
        }
        // Copy basic python setup
        copyPySetup()
        substituteVersionsInConda()
        substituteInSetup()
        def replaceStr =
                """
import zipfile
from os import path

if '.zip' in here:
    try:
        with zipfile.ZipFile(path.dirname(here), 'r') as archive:
            __buildinfo__ = archive.read('h2o/buildinfo.txt').decode('utf-8').strip()
    except:
        __buildinfo__ = "unknown"
else:
    try:
        with open(path.join(here, 'buildinfo.txt'), encoding='utf-8') as f:
            __buildinfo__ = f.read().strip()
    except:
         __buildinfo__ = "unknown"

if '.zip' in here:
    try:
        with zipfile.ZipFile(path.dirname(here), 'r') as archive:
            __version__ = archive.read('h2o/version.txt').decode('utf-8').strip()
    except:
        __version__ = "0.0.local"
else:
    try:
        with open(path.join(here, 'version.txt'), encoding='utf-8') as f:
            __version__ = f.read().strip()
    except:
         __version__ = "0.0.local"
                """

        // Change __init__.py in H2O to be aware of being zipped
        def initFile = file("${project.pkgDir}/h2o/__init__.py")
        def initContent = initFile.readLines()
        def lineStart = initContent.findIndexOf {it == "try:"}
        def lineEnd = initContent.findIndexOf {it == "    __version__ = \"0.0.local\""}
        if (lineStart != -1 && lineEnd != -1){
            def before = initContent.subList(0, lineStart).join("\n")
            def full = before + replaceStr + initContent.subList(lineEnd + 1, initContent.size()).join("\n")
            initFile.write(full)
        }


        // Copy sparkling water fatjar
        def fatJarName = "sparkling-water-assembly_${scalaBaseVersion}-${version}-all.jar"
        def fatJar = configurations.compile.filter{
            it.absoluteFile.getName() == fatJarName}.singleFile
        copy {
            from fatJar
            into  file("${project.pkgDir}/sparkling_water")
            rename ".*", "sparkling_water_assembly.jar"
        }
        // Save comment into module file
        file("${project.pkgDir}/sparkling_water/").mkdir()
        file("${project.pkgDir}/sparkling_water/__init__.py").write("# Sparkling-water JAR holder for pySparkling module.")
    }
    // Configure proper name
    baseName = "h2o_pysparkling_${version.substring(0, version.lastIndexOf('.'))}"

    from pkgDir
    destinationDir distDir
}

configurations{
    sdist
}

artifacts {
    sdist distPython
}

task testPythonConf(type: Exec, dependsOn: [distPython, pipInstall]){
    preparePythonTestEnv(environment)
    commandLine getOsSpecificCommandLine([pythonexec, "tests/tests_unit_conf.py"])
}

task testPythonConversions(type: Exec, dependsOn: [distPython, pipInstall]){
    preparePythonTestEnv(environment)
    commandLine getOsSpecificCommandLine([pythonexec, "tests/tests_unit_conversions.py"])
}

task testPythonMojoPredictions(type: Exec, dependsOn: [distPython, pipInstall]){
    preparePythonTestEnv(environment)
    commandLine getOsSpecificCommandLine([pythonexec, "tests/tests_unit_mojo_predictions.py"])
}


task testPythonMojoPipeline(type: Exec, dependsOn: [distPython, pipInstall]) {
    preparePythonTestEnv(environment)
    commandLine getOsSpecificCommandLine([pythonexec, "tests/tests_unit_mojo_pipeline.py"])
}

task hadoopSmokeTests(type: Exec, dependsOn: [distPython, pipInstall]){
    preparePythonTestEnv(environment)
    commandLine getOsSpecificCommandLine([pythonexec, "tests/tests_hadoop_smoke.py"])
}

//
// Test python against Spark cloud
//
task testPython(dependsOn: [testPythonConf, testPythonConversions, testPythonMojoPredictions]) {}

if (project.property("testMojoPipeline") == "true") {
 testPython.dependsOn testPythonMojoPipeline
}

//
// Run python integration tests
//
task integTestPython(type: Exec, dependsOn: [distPython, pipInstall]) {
    def testEnv = detectTestEnvironment()

    // Pass references to libraries to test launcher
    environment["sparkling.pysparkling.sdist"] = configurations.sdist.artifacts.files.singleFile.absolutePath
    environment['spark.ext.h2o.backend.cluster.mode'] = detectBackendClusterMode()
    environment["spark.ext.h2o.external.start.mode"] = detectExternalBackendStartMode()
    // sparkling assembly jar is here because of external h2o tests
    environment["sparkling.assembly.jar"] = configurations.compile.join(',')
    environment["spark.testing"] = "true"
    environment["spark.test.home"] = "${sparkHome}"
    environment["sparkling.test.hdp.version"] = "${hdpVersion}"


    if (sparkMaster != null)
        environment["spark.master"] = "${sparkMaster}"

    // Decide which tests should be launch here based on environment
    switch (testEnv) {
        case "yarn":
            environment["sparkling.test.environment"] = "${testEnv}"
            environment["spark.ext.h2o.node.log.dir"] = "h2ologs-itest-${testEnv}/nodes"
            environment["spark.ext.h2o.client.log.dir"] =  "h2ologs-itest-${testEnv}/client"

            commandLine getOsSpecificCommandLine([pythonexec, "tests/tests_integ_yarn.py"])
            break

        case "standalone":
            environment["sparkling.test.environment"] = "${testEnv}"
            environment["spark.ext.h2o.node.log.dir"] = "h2ologs-itest-${testEnv}/nodes"
            environment["spark.ext.h2o.client.log.dir"] = "h2ologs-itest-${testEnv}/client"

            commandLine getOsSpecificCommandLine([pythonexec, "tests/tests_integ_standalone.py"])
            break

        case "local":
            environment["sparkling.test.environment"] = "${testEnv}"
            environment["spark.ext.h2o.node.log.dir"] = new File(project.getBuildDir(), "h2ologs-itest-${testEnv}/nodes").getAbsolutePath()
            environment["spark.ext.h2o.client.log.dir"] = new File(project.getBuildDir(), "h2ologs-itest-${testEnv}/client").getAbsolutePath()

            commandLine getOsSpecificCommandLine([pythonexec, "tests/tests_integ_local.py"])
            break
    }
}

// Run integration tests as part of build
task integTest
integTest.dependsOn integTestPython
check.dependsOn integTest

//
// Cleanup
//
task cleanPython(type: Delete) {
    delete getBuildDir()
}

//
// Just print location of H2O Python Wheel package with respect to a configured version of H2O dependency
//
task printH2OWheelPackage {
    doLast {
        description = "Print location of H2O Python Wheel package for download"
        println(h2oPythonWheelPackageLocation)
    }
}

//
// Setup execution graph
//
clean.dependsOn cleanPython
createVersionFile.dependsOn cleanPython
distPython.dependsOn createVersionFile
 
// Build tasks
task buildPython(dependsOn: distPython)
build.dependsOn buildPython

test.dependsOn testPython





