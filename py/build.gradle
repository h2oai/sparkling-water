description = "PySparkling - Sparkling Water Python Package"

apply from: "$rootDir/gradle/utils.gradle"
apply plugin: 'ru.vyarus.use-python'

import ru.vyarus.gradle.plugin.python.cmd.Python

def getPythonVersion() {
    Python p = new Python(project, python.getPythonPath(), python.getPythonBinary())
    return p.version
}

ext {
    FS = File.separator
    FPS = File.pathSeparator
    pythonBinary = findProperty("pythonBinary") ?: "python"
    pkgDir = file("$buildDir/pkg")
    distDir = file("$buildDir/dist")
    condaDir = file("$buildDir/conda/h2o_pysparkling_${sparkMajorVersion}")
    wheelFile = file("${rootDir}/.gradle/wheels/h2o-${h2oMajorVersion}.${h2oBuild}-py2.py3-none-any.whl")
}

// Define the environment required to run tests
python {
    if (project.hasProperty("pythonBinary")) {
        pythonBinary project.findProperty("pythonBinary").toString()
    }
    if (project.hasProperty("pythonPath")) {
        pythonPath project.findProperty("pythonPath").toString()
    }

    pip "pytz:2019.1" // Needed in Integration tests, but not PySparkling dependency
    pip "tabulate:0.8.3"
    pip "requests:2.21.0"
    pip "future:0.17.1"
    pip "colorama:0.3.8"
    pip "numpy:1.9.2"
    pip "pyspark:${sparkVersion}"
    if (project.hasProperty("pythonEnvBasePath")) {
        // for CI as we use pre-cached environment
        envPath "${project.findProperty("pythonEnvBasePath")}/${getPythonVersion()}/${sparkVersion}"
    } else {
        envPath "${rootDir}/.gradle/python/${getPythonVersion()}/${sparkVersion}"
    }
}

configurations {
    mojopipeline
    s3deps
    sparklingWaterAssemblyJar
}

dependencies {
    sparklingWaterAssemblyJar project(path: ':sparkling-water-assembly', configuration: 'shadow')

    if (project.property("testMojoPipeline") == "true") {
        mojopipeline "ai.h2o:mojo2-runtime-impl:${mojoPipelineVersion}"
    }

    s3deps "com.amazonaws:aws-java-sdk:1.7.4"
    s3deps "org.apache.hadoop:hadoop-aws:2.7.3"
}

task resolveTestRuntimeDependencies {
    doLast {
        project.configurations.mojopipeline.resolve()
    }
}

//
// Create a file with version for Python dist task
//
task createVersionFile {
    doLast {
        def versionFileDir = new File(pkgDir, "pysparkling")
        if (!versionFileDir.exists()) {
            versionFileDir.mkdirs()
        }
        File version_file = new File(versionFileDir, "version.txt")

        def version_txt = version
        version_file.write(version_txt)
    }
}

//
// Represents a location of H2O Wheel Package
//
final String h2oPythonWheelPackageLocation = "http://h2o-release.s3.amazonaws.com/h2o/${h2oMajorName != "master" ? "rel-${h2oMajorName}" : "master"}/${h2oBuild}/Python/h2o-${h2oMajorVersion}.${h2oBuild}-py2.py3-none-any.whl"


static def downloadH2OWheel(String url, File out) {
    if (!out.parentFile.exists()) {
        out.parentFile.mkdirs()
    }

    if (!out.exists()) {
        new URL(url).withInputStream { i -> out.withOutputStream { it << i } }
    }
}

//
// Initial task checking setup of all properties required
// by Python build
//
task checkPythonEnv {
    doLast {
        def H2O_HOME = System.getenv("H2O_HOME")
        def H2O_EXTENDED_JAR = System.getenv("H2O_EXTENDED_JAR")

        if (H2O_HOME == null) {
            downloadH2OWheel(h2oPythonWheelPackageLocation, wheelFile)
        } else {
            logger.info("Using \"H2O_HOME\" to specify H2O python package location!")
        }

        // if the spark.ext.h2o.backend.cluster.mode is set to external, then
        // we need to have also H2O_EXTENDED_JAR property set in order to be able to perform tests
        if (detectBackendClusterMode() == "external" && H2O_EXTENDED_JAR == null) {
            throw new InvalidUserDataException("""When running tests on external H2O cluster, H2O_EXTENDED_JAR property is required.

Please set it, for example as:

export H2O_EXTENDED_JAR=`./gradlew -q extendJar -PdownloadH2O`
                                  """)
        }

    }
}

def copyPySetup() {

    copy {
        from("$projectDir") {
            include 'setup.py'
        }
        filter {
            it.replaceAll("SUBST_SPARK_MAJOR_VERSION", sparkMajorVersion)
                    .replaceAll("SUBST_SPARK_VERSION", sparkVersion)
                    .replaceAll("SUBST_SW_VERSION", version.substring(0, version.lastIndexOf("-")))
                    .replaceAll("SUBST_SPARK_MAJOR_VERSION", sparkMajorVersion)
        }
        into pkgDir
    }

    copy {
        from("$projectDir") {
            include 'README.rst'
            include 'MANIFEST.in'
            include 'setup.cfg'
            include 'pysparkling/**/*'
            include 'py_sparkling/**/*'
            include 'ai/**/*'
            exclude '**/*.pyc'
        }
        into pkgDir
    }

    copy {
        from("$projectDir/conda/h2o_pysparkling_SUBST_SPARK_MAJOR_VERSION") {
            include 'bld.bat'
            include 'build.sh'
        }
        into condaDir
    }

    copy {
        from("$projectDir/conda/h2o_pysparkling_SUBST_SPARK_MAJOR_VERSION") {
            include 'meta.yaml'
        }
        filter {
            it.replaceAll("SUBST_SPARK_MAJOR_VERSION", sparkMajorVersion)
                    .replaceAll("SUBST_SPARK_VERSION", sparkVersion)
                    .replaceAll("SUBST_SW_VERSION", version.substring(0, version.lastIndexOf("-")))
        }
        into condaDir
    }
}


def copyH2OFromH2OHome(String h2oHome) {
    copy {
        from "${h2oHome}/h2o-py/h2o"
        into file("${project.pkgDir}/h2o")
        exclude '**/*.pyc'
        exclude '**/h2o.jar'
    }
}

def copyH2OFromH2OWheel(File h2oPythonWheel) {
    copy {
        from zipTree(h2oPythonWheel)
        into file("${project.pkgDir}")
        include 'h2o/**'
        exclude '**/*.pyc'
        exclude '**/h2o.jar'
    }
}

//
// Make PySparkling distribution zip package
//
task distPython(type: Zip, dependsOn: [checkPythonEnv, configurations.sparklingWaterAssemblyJar]) {

    doFirst {
        def H2O_HOME = System.getenv("H2O_HOME")

        // if both properties are set, give precedence to H2O_HOME
        if (H2O_HOME != null) {
            copyH2OFromH2OHome(H2O_HOME)
        } else {
            copyH2OFromH2OWheel(wheelFile)
        }
        // Copy basic python setup
        copyPySetup()
        def replaceStr =
                """
import zipfile
from os import path

if '.zip' in here:
    try:
        with zipfile.ZipFile(path.dirname(here), 'r') as archive:
            __buildinfo__ = archive.read('h2o/buildinfo.txt').decode('utf-8').strip()
    except:
        __buildinfo__ = "unknown"
else:
    try:
        with open(path.join(here, 'buildinfo.txt'), encoding='utf-8') as f:
            __buildinfo__ = f.read().strip()
    except:
         __buildinfo__ = "unknown"

if '.zip' in here:
    try:
        with zipfile.ZipFile(path.dirname(here), 'r') as archive:
            __version__ = archive.read('h2o/version.txt').decode('utf-8').strip()
    except:
        __version__ = "0.0.local"
else:
    try:
        with open(path.join(here, 'version.txt'), encoding='utf-8') as f:
            __version__ = f.read().strip()
    except:
         __version__ = "0.0.local"
                """

        // Change __init__.py in H2O to be aware of being zipped
        def initFile = file("${project.pkgDir}/h2o/__init__.py")
        def initContent = initFile.readLines()
        def lineStart = initContent.findIndexOf { it == "try:" }
        def lineEnd = initContent.findIndexOf { it == "    __version__ = \"0.0.local\"" }
        if (lineStart != -1 && lineEnd != -1) {
            def before = initContent.subList(0, lineStart).join("\n")
            def full = before + replaceStr + initContent.subList(lineEnd + 1, initContent.size()).join("\n")
            initFile.write(full)
        }

        // Copy sparkling water assembly jar
        def fatJar = configurations.sparklingWaterAssemblyJar.singleFile
        copy {
            from fatJar
            into file("${project.pkgDir}/sparkling_water")
            rename ".*", "sparkling_water_assembly.jar"
        }
        // Save comment into module file
        file("${project.pkgDir}/sparkling_water/").mkdir()
        file("${project.pkgDir}/sparkling_water/__init__.py").write("# Sparkling-water JAR holder for pySparkling module.")
    }
    // Configure proper name
    archiveBaseName = "h2o_pysparkling_${sparkMajorVersion}"

    from pkgDir
    destinationDir distDir
}

configurations {
    sdist
}

artifacts {
    sdist distPython
}

def createUnitTestArgs() {
    def args = [
            "${distPython.archiveFile.get()}",
            "spark.ext.h2o.backend.cluster.mode=${detectBackendClusterMode()}"
    ]

    if (project.property("testMojoPipeline") == "true") {
        args.add("spark.jars=${configurations.mojopipeline.resolve().join(",")}")
    }

    gradle.taskGraph.whenReady {
        if (gradle.taskGraph.hasTask(":${project.name}:hadoopSmokeTests")) {
            args.add("spark.jars=${configurations.s3deps.resolve().join(",")}")
        }
    }
    return args
}

def createIntegTestArgs(String integTestKind) {
    return [
            "${distPython.archiveFile.get()}",
            "spark.testing=true",
            "spark.ext.h2o.backend.cluster.mode=${detectBackendClusterMode()}",
            "spark.test.home=${sparkHome}",
            "spark.ext.h2o.node.log.dir=${buildDir}/h2ologs-itest-${integTestKind}/nodes",
            "spark.ext.h2o.client.log.dir=${buildDir}/h2ologs-itest-${integTestKind}/client",
            "spark.driver.extraJavaOptions=-Dhdp.version=${hdpVersion} ${extraJVMArgs}",
            "spark.executor.extraJavaOptions=-Dhdp.version=${hdpVersion}",
            "spark.yarn.am.extraJavaOptions=-Dhdp.version=${hdpVersion}"
    ]
}

task testPythonConf(type: PythonTask, dependsOn: distPython) {
    extraArgs(*createUnitTestArgs())
    command = "tests/tests_unit_conf.py"
}

task testPythonConversions(type: PythonTask, dependsOn: distPython) {
    extraArgs(*createUnitTestArgs())
    command = "tests/tests_unit_conversions.py"
}

task testPythonMojoPredictions(type: PythonTask, dependsOn: distPython) {
    extraArgs(*createUnitTestArgs())
    command = "tests/tests_unit_mojo_predictions.py"
}

task testPythonMojoPipeline(type: PythonTask, dependsOn: distPython) {
    extraArgs(*createUnitTestArgs())
    command = "tests/tests_unit_mojo_pipeline.py"
}

task hadoopSmokeTests(type: PythonTask, dependsOn: distPython) {
    extraArgs(*createUnitTestArgs())
    command = "tests/tests_hadoop_smoke.py"
}

task testPython(dependsOn: [testPythonConf, testPythonConversions, testPythonMojoPredictions]) {}

if (project.property("testMojoPipeline") == "true") {
    testPython.dependsOn testPythonMojoPipeline
}

task localIntegTestsPython(type: PythonTask, dependsOn: [distPython, checkSparkVersionTask]) {
    extraArgs(*createIntegTestArgs("local"))
    command = "tests/tests_integ_local.py"
}


task yarnIntegTestsPython(type: PythonTask, dependsOn: [distPython, checkSparkVersionTask]) {
    extraArgs(*createIntegTestArgs("yarn"))
    command = "tests/tests_integ_yarn.py"
}

task integTestPython(dependsOn: localIntegTestsPython)
//
// Just print location of H2O Python Wheel package with respect to a configured version of H2O dependency
//
task printH2OWheelPackage {
    doLast {
        description = "Print location of H2O Python Wheel package for download"
        println(h2oPythonWheelPackageLocation)
    }
}

//
// Cleanup
//
task cleanPython(type: Delete) {
    delete getBuildDir()
}

//
// Setup execution graph
//
clean.dependsOn cleanPython
createVersionFile.dependsOn clean
distPython.dependsOn createVersionFile

build.dependsOn distPython
test.dependsOn testPython
integTest.dependsOn integTestPython

