description = "Sparkling Water Core"

dependencies {
  // Required for h2o-app (we need UI)
  compile("ai.h2o:h2o-app:${h2oVersion}") {
      //
      // Exclude all dependencies provided by Spark environment already
      // The motivation is to catch error at compile time
      exclude(group: "net.java.dev.jets3t", module: "jets3t")
      exclude(group: "commons-collections", module: "commons-collections")
  }

  // H2O Scala API
  compile "ai.h2o:h2o-scala_${scalaBinaryVersion}:${h2oVersion}"
  // H2O Persistent layer
  compile("ai.h2o:h2o-persist-hdfs:${h2oVersion}") {
      // Cannot use here: transitive = false since Gradle is producing wrong POM file
      // Hence the exlusions are listed manually
      exclude(group: "org.apache.hadoop", module: "hadoop-client")
  }

  // Spark 1.2.0 release
  // - core
  compile "org.apache.spark:spark-core_${scalaBinaryVersion}:${sparkVersion}"
  // - SQL component
  compile "org.apache.spark:spark-sql_${scalaBinaryVersion}:${sparkVersion}"
  // - MLLib component
  compile "org.apache.spark:spark-mllib_${scalaBinaryVersion}:${sparkVersion}"
  // - Spark REPL
  compile "org.apache.spark:spark-repl_${scalaBinaryVersion}:${sparkVersion}"

  // Add joda optional convert library which is required in Scala environment
  compile "org.joda:joda-convert:1.7"

  // And Scala library
  compile "org.scala-lang:scala-library:${scalaVersion}"

  // And use scalatest for Scala testing
  testCompile "org.scalatest:scalatest_${scalaBinaryVersion}:2.2.1"
  testCompile "junit:junit:4.11"
}

// Add a new configuration containing only jars needed for test run
configurations {
    testAssembly {
        description = 'Contains only jars needed to run tests on top of Spark.'
        transitive = false
    }
}

//
// Setup artifacts for testing on top of Spark cluster
// - should contain all dependencies required to build an assembly executable on Spark
artifacts {
    testAssembly jar
    testAssembly testJar

    // pull selected dependencies
    dep(testDependencies).each {
        testAssembly it
    }
}

// Setup test environment for Spark
test {
    // Test environment
    systemProperty "spark.testing",   "true"

    systemProperty "spark.test.home", "${sparkHome}"
    // Pass list of jars required for testing
    systemProperty "sparkling.test.assembly", "${configurations.testAssembly.artifacts.file.join(',')}"

    // Run with assertions ON
    enableAssertions = true

    // For a new JVM for each test class
    forkEvery = 1

    jvmArgs '-XX:MaxPermSize=384m'
}

test.dependsOn testJar

/**
 * Find dependencies from 'configuration.runtime' matching given specification.
 *
 * Specification is given in form group_name:module_name
 *
 * @param d list of specifications.
 * @return
 */
public FileCollection dep(List<String> d) {
    Configuration conf = configurations.runtime
    List<String[]> depsSpec = d.collect { it.split(":")}
    Set<ResolvedDependency> toInclude = []
    resolve(conf.resolvedConfiguration.firstLevelModuleDependencies, toInclude, depsSpec)
    files( toInclude.collect {
        it.moduleArtifacts*.file
    }.flatten())
}

void resolve(Set<ResolvedDependency> deps, Set<ResolvedDependency> includeDeps, List<String[]> depsSpec) {
    deps.each { d->
        if (depsSpec.any { ds -> d.moduleGroup==ds[0] && d.moduleName==ds[1] })
            if (includeDeps.add(d)) {
                resolve(d.children, includeDeps, depsSpec)
            }
    }
}
