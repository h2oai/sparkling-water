description = "Sparkling Water Core"

apply from: "$rootDir/gradle/utils.gradle"
apply from: "$rootDir/gradle/sparkTest.gradle"

dependencies {
  // Required for h2o-app (we need UI)
  compile("ai.h2o:h2o-app:${h2oVersion}") {
      //
      // Exclude all dependencies provided by Spark environment already
      // The motivation is to catch error at compile time
      exclude(group: "net.java.dev.jets3t", module: "jets3t")
      exclude(group: "commons-collections", module: "commons-collections")
  }

  // H2O Scala API
  compile "ai.h2o:h2o-scala_${scalaBaseVersion}:${h2oVersion}"
  // H2O Persistent layer
  compile("ai.h2o:h2o-persist-hdfs:${h2oVersion}") {
      // Cannot use here: transitive = false since Gradle is producing wrong POM file
      // Hence the exlusions are listed manually
      exclude(group: "org.apache.hadoop", module: "hadoop-client")
  }

  compile("ai.h2o:h2o-persist-s3:${h2oVersion}") {
      exclude(group: "com.fasterxml.jackson.core")
  }

  // Spark components
  // - core
  compile "org.apache.spark:spark-core_${scalaBaseVersion}:${sparkVersion}"
  // - SQL component
  compile "org.apache.spark:spark-sql_${scalaBaseVersion}:${sparkVersion}"
  // - MLLib component
  compile "org.apache.spark:spark-mllib_${scalaBaseVersion}:${sparkVersion}"

  // Sparkling Water REPL
  compile(project(':sparkling-water-repl'))

  // Add joda optional convert library which is required in Scala environment
  compile "org.joda:joda-convert:1.7"

  // And Scala library
  compile "org.scala-lang:scala-library:${scalaVersion}"

  // And use scalatest for Scala testing
  testCompile "org.scalatest:scalatest_${scalaBaseVersion}:2.2.1"
  testCompile "junit:junit:4.11"

  // Integration tests requirements
  integTestCompile "org.scalatest:scalatest_${scalaBaseVersion}:2.2.1"
  integTestCompile "junit:junit:4.11"

  // Put Spark Assembly on runtime path
  integTestRuntime fileTree(dir: new File((String) sparkHome, "lib/"), include: '*.jar' )
}

task createSparkVersionFile {
    doLast {
        File version_file = file("src/main/resources/spark.version")
        // Create parent directories if not created yet
        version_file.getParentFile().mkdirs()
        version_file.write(sparkVersion)
    }
}

test.dependsOn testJar
processResources.dependsOn createSparkVersionFile

integTest {
    // Pass references to libraries to test launcher
    systemProperty "spark.ext.h2o.backend.cluster.mode", detectBackendClusterMode()
    systemProperty "spark.testing",   "true"
    systemProperty "spark.test.home", "${sparkHome}"
    systemProperty "sparkling.test.hdp.version", "${hdpVersion}"

    // if this property is set up h2o cluster will be started on yarn instead on local machine
    if(project.hasProperty("startH2OClusterOnYarn")){
        systemProperty "spark.ext.h2o.external.start.mode", "auto"
    }

    // Pass list of jars required for testing
    systemProperty "sparkling.assembly.jar", "${project(":sparkling-water-assembly").configurations.shadow.artifacts.file.join(',')}"
    systemProperty "sparkling.itest.jar", "${integTestJar.archivePath}"
}