description = "Sparkling Water Core"

apply from: "$rootDir/gradle/utils.gradle"

dependencies {
  // Required for h2o-app (we need UI)
  compile("ai.h2o:h2o-app:${h2oVersion}") {
      //
      // Exclude all dependencies provided by Spark environment already
      // The motivation is to catch error at compile time
      exclude(group: "net.java.dev.jets3t", module: "jets3t")
      exclude(group: "commons-collections", module: "commons-collections")
  }

  // H2O Scala API
  compile "ai.h2o:h2o-scala_${scalaBinaryVersion}:${h2oVersion}"
  // H2O Persistent layer
  compile("ai.h2o:h2o-persist-hdfs:${h2oVersion}") {
      // Cannot use here: transitive = false since Gradle is producing wrong POM file
      // Hence the exlusions are listed manually
      exclude(group: "org.apache.hadoop", module: "hadoop-client")
  }

  // Spark 1.2.0 release
  // - core
  compile "org.apache.spark:spark-core_${scalaBinaryVersion}:${sparkVersion}"
  // - SQL component
  compile "org.apache.spark:spark-sql_${scalaBinaryVersion}:${sparkVersion}"
  // - MLLib component
  compile "org.apache.spark:spark-mllib_${scalaBinaryVersion}:${sparkVersion}"
  // - Spark REPL
  compile "org.apache.spark:spark-repl_${scalaBinaryVersion}:${sparkVersion}"

  // Add joda optional convert library which is required in Scala environment
  compile "org.joda:joda-convert:1.7"

  // And Scala library
  compile "org.scala-lang:scala-library:${scalaVersion}"

  // And use scalatest for Scala testing
  testCompile "org.scalatest:scalatest_${scalaBinaryVersion}:2.2.1"
  testCompile "junit:junit:4.11"
}

// Add a new configuration containing only jars needed for test run
configurations {
    testAssembly {
        description = 'Contains only jars needed to run tests on top of Spark.'
        transitive = false
    }
}

//
// Setup artifacts for testing on top of Spark cluster
// - should contain all dependencies required to build an assembly executable on Spark
artifacts {
    testAssembly jar
    testAssembly testJar

    // pull selected dependencies
    dep(testDependencies).each {
        testAssembly it
    }
}

// Setup test environment for Spark
test {
    // Test environment
    systemProperty "spark.testing",   "true"

    systemProperty "spark.test.home", "${sparkHome}"
    // Pass list of jars required for testing
    systemProperty "sparkling.test.assembly", "${configurations.testAssembly.artifacts.file.join(',')}"

    systemProperty "spark.ext.h2o.node.log.dir", new File(project.getBuildDir(), "h2ologs-test")
    systemProperty "spark.ext.h2o.client.log.dir", new File(project.getBuildDir(), "h2ologs-test")

    // Run with assertions ON
    enableAssertions = true

    // For a new JVM for each test class
    forkEvery = 1

    // Increase heap size
    maxHeapSize = "4g"

    // Increase PermGen
    jvmArgs '-XX:MaxPermSize=384m'

    // Working dir will be root project
    workingDir = rootDir
}
test.dependsOn testJar
