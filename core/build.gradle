description = "Sparkling Water Core"

dependencies {
  // Required for h2o-app (we need UI)
  compile "ai.h2o:h2o-app:${h2oVersion}"
  // H2O Scala API
  compile "ai.h2o:h2o-scala_${scalaBinaryVersion}:${h2oVersion}"

  // Spark 1.1.0 release
  // - core
  compile "org.apache.spark:spark-core_${scalaBinaryVersion}:${sparkVersion}"
  // - SQL component
  compile "org.apache.spark:spark-sql_${scalaBinaryVersion}:${sparkVersion}"

  // And Scala library
  compile "org.scala-lang:scala-library:${scalaVersion}"

  // And use scalatest for Scala testing
  testCompile "org.scalatest:scalatest_${scalaBinaryVersion}:2.2.1"
  testCompile "junit:junit:4.11"
}

// Add a new configuration containing only jars needed for test run
configurations {
    testAssembly {
        description = 'Contains only jars needed to run tests on top of Spark.'
        transitive = false
    }
}

artifacts {
    testAssembly jar
    testAssembly testJar

    // pull selected dependencies
    dep(["ai.h2o:h2o-core",
     "ai.h2o:h2o-algos",
     "ai.h2o:h2o-app",
     "joda-time:joda-time",
     "org.javassist:javassist",
     "gov.nist.math:jama",
     "com.google.code.gson:gson",
     "org.reflections:reflections"]).each {
        testAssembly it
    }
}

// Setup test environment for Spark
test {
    // Test environment
    systemProperty "spark.testing",   "true"

    def sh = getSparkHome()
    if (sh!=null) systemProperty "spark.test.home", "${sh}"
    // Pass list of jars required for testing
    systemProperty "sparkling.test.assembly", "${configurations.testAssembly.artifacts.file.join(',')}"


    // Run with assertions ON
    enableAssertions = true

    // For a new JVM for each test class
    forkEvery = 1

    // Make sure that test driver have enough memory
    maxHeapSize = "2g"
}

test.dependsOn testJar

// Useful task to setup idea test configuration
task printTestEnv << {
    println("-Dspark.testing=true\n" +
            "-Dspark.test.home=${getSparkHome()}\n" +
            "-Dsparkling.test.assembly=${configurations.testAssembly.artifacts.file.join(',')}\n")
}

/* Get spark home value from local environmnet */
String getSparkHome() {
    String sh =
        [ project.hasProperty("sparkHome") ? project["sparkHome"] : null,
          System.properties["master"],
          System.env['SPARK_HOME']
        ].find { h -> h!=null ? new File(h).isDirectory() : false }
    if (sh!=null) {
        logger.info("Found spark home: ${sh}")
        return sh
    } else {
        logger.warn("Cannot find Spark home - " +
                "please specify SPARK_HOME env variable or pass -PsparkHome property to gradle script")
        return null
    }
}

/**
 * Find dependencies from 'configuration.runtime' matching given specification.
 *
 * Specification is given in form group_name:module_name
 *
 * @param d list of specifications.
 * @return
 */
FileCollection dep(List<String> d) {
    Configuration conf = configurations.runtime
    List<String[]> depsSpec = d.collect { it.split(":")}
    Set<ResolvedDependency> toInclude = []
    resolve(conf.resolvedConfiguration.firstLevelModuleDependencies, toInclude, depsSpec)
    files( toInclude.collect {
        it.moduleArtifacts*.file
    }.flatten())
}
void resolve(Set<ResolvedDependency> deps, Set<ResolvedDependency> includeDeps, List<String[]> depsSpec) {
    deps.each { d->
        if (depsSpec.any { ds -> d.moduleGroup==ds[0] && d.moduleName==ds[1] })
            if (includeDeps.add(d)) {
                resolve(d.children, includeDeps, depsSpec)
            }
    }
}
